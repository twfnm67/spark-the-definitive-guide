{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a6dcec-0c84-4537-a728-a683d2fff23e",
   "metadata": {},
   "source": [
    "# Chapter13. RDD 고급 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d1e46e-58f5-4098-88d2-07ea83e79778",
   "metadata": {},
   "source": [
    "- 집계와 키-값 형태의 RDD\n",
    "- 사용자 정의 파티셔닝\n",
    "- RDD 조인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "726c6398-1991-47f5-8846-87d975ecc3ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.2:4040\n",
       "SparkContext available as 'sc' (version = 3.3.2, master = local[*], app id = local-1686093283879)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3aaf14d6\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "986b36fa-a920-4b1b-a40d-947e92f8f735",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myCollection: Array[String] = Array(Spark, The, Definitive, Guide, :, Big, Data, Processing, Made, Simple)\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "121c3b3a-a4c9-49cc-8d6f-06d1192dcb59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:24\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val words = spark.sparkContext.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7af0ef-1a5d-41f6-ac5e-3127fef4ec98",
   "metadata": {},
   "source": [
    "# 13.1 키-값 형태의 기초(키-값 형태의 RDD)\n",
    "- RDD에는 데이터를 키-값 형태로 다룰 수 있는 다양한 메서드가 있음\n",
    "    - 이러한 메서드는 <연산명>ByKey 형태의 이름을 가짐\n",
    "    - 메서드 이름에 ByKey가 있다면 PairRDD 타입만 사용할 수 있음\n",
    "    - PairRDD 타입을 만드는 가장 쉬운 방법은 RDD에 맵 연산을 수행해 키-값 구조로 만드는 것\n",
    "    - 즉, 레코드에 두 개의 값이 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "704c0b98-31b2-4f82-8075-c43709c7294b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[1] at map at <console>:26\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(word => (word.toLowerCase, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e398460a-e48a-4c03-9991-a3f8e36d51b4",
   "metadata": {},
   "source": [
    "## 13.1.1 keyBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b28ad28-b42a-4b2c-921a-6883e78ded85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keyword: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[2] at keyBy at <console>:24\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val keyword = words.keyBy(word => word.toLowerCase.toSeq(0).toString)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90259f16-d30e-4b2a-8fa0-edb1b2a27c97",
   "metadata": {},
   "source": [
    "## 13.1.2 값 매핑하기\n",
    "- 만약 튜플 형태의 데이터를 사용한다면 스파크는 튜플의 첫 번째 요소를 키로, 두 번째 요소를 값으로 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7193556-45ef-4db6-b58d-903152334ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[(String, String)] = Array((s,SPARK), (t,THE), (d,DEFINITIVE), (g,GUIDE), (:,:), (b,BIG), (d,DATA), (p,PROCESSING), (m,MADE), (s,SIMPLE))\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.mapValues(word => word.toUpperCase).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53515852-8b36-40c1-b082-b4169a1dd9dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[(String, Char)] = Array((s,S), (s,P), (s,A), (s,R), (s,K), (t,T), (t,H), (t,E), (d,D), (d,E), (d,F), (d,I), (d,N), (d,I), (d,T), (d,I), (d,V), (d,E), (g,G), (g,U), (g,I), (g,D), (g,E), (:,:), (b,B), (b,I), (b,G), (d,D), (d,A), (d,T), (d,A), (p,P), (p,R), (p,O), (p,C), (p,E), (p,S), (p,S), (p,I), (p,N), (p,G), (m,M), (m,A), (m,D), (m,E), (s,S), (s,I), (s,M), (s,P), (s,L), (s,E))\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//flatmap 함수를 사용해 반환되는 결과의 각 로우가 문자를 나타내도록 확장할 수 있음\n",
    "keyword.flatMapValues(word => word.toUpperCase).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e2967e-1625-40d2-a398-a0b59cfc885b",
   "metadata": {},
   "source": [
    "## 13.1.3 키와 값 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33ee0470-6152-4335-9ebe-e50292e179a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[String] = Array(s, t, d, g, :, b, d, p, m, s)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.keys.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1119e6ce-ab10-4137-9cf1-3fe8c444dad6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Array[String] = Array(Spark, The, Definitive, Guide, :, Big, Data, Processing, Made, Simple)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.values.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9311e244-2b4f-45f7-8797-d925d47379de",
   "metadata": {},
   "source": [
    "## 13.1.4 lookup\n",
    "- 특정 키에 관한 결과를 찾기\n",
    "    - 그러나 각 입력에 대해 오직 하나의 키만 찾을 수 있도록 강제하는 메커니즘은 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ead087aa-dd49-4dc4-bdef-06d21484b1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Seq[String] = WrappedArray(Spark, Simple)\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.lookup(\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e30ed5-ff13-44be-b273-c479dcf0b98a",
   "metadata": {},
   "source": [
    "## 13.1.5 sampleByKey\n",
    "- 근사치나 정확도를 이용해 키를 기반으로 RDD 샘플을 생성할 수 있음\n",
    "- 특정 키를 부분 샘플링할 수 있으며 선택에 따라 비복원 추출을 사용할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10faff43-5491-422a-be26-e5cf05886e99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distinctChars: Array[Char] = Array(d, p, t, b, h, n, f, v, :, r, l, s, e, a, i, k, u, o, g, m, c)\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val distinctChars = words.flatMap(word => word.toLowerCase.toSeq).distinct.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad9dca43-ca99-4816-9c21-cc2daf707f40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.util.Random\n",
       "sampleMap: scala.collection.immutable.Map[Char,Double] = Map(e -> 0.4375463899951435, s -> 0.6673301150653165, n -> 0.673849374110227, t -> 0.5510348197633197, u -> 0.6174184873558151, f -> 0.7642923286632328, a -> 0.658489020265458, m -> 0.2896779594638964, i -> 0.6882052046882807, v -> 0.3032258433656986, b -> 0.5462155668570206, g -> 0.83621646234217, l -> 0.22473109345403064, p -> 0.4024886231352476, c -> 0.4982075442532208, h -> 0.2973042221059691, r -> 0.126795826003682, : -> 0.5812962361524623, k -> 0.9418188180232476, o -> 0.7479822120139356, d -> 0.22010257052046878)\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Random\n",
    "\n",
    "val sampleMap = distinctChars.map(c => (c, new Random().nextDouble())).toMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52f78afc-044f-4ae0-b2ae-1926888606c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Array[(Char, String)] = Array((s,Spark), (t,The), (d,Definitive), (g,Guide), (:,:))\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(word => (word.toLowerCase.toSeq(0), word)).sampleByKey(true, sampleMap, 6L).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "616ac6a0-cce2-416c-80d9-5d3f72b1f56c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Array[(Char, String)] = Array((s,Spark), (t,The), (d,Definitive), (g,Guide), (:,:), (b,Big), (p,Processing), (m,Made), (s,Simple))\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(word => (word.toLowerCase.toSeq(0), word)).sampleByKeyExact(true, sampleMap, 6L).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48b9481-311d-46ee-b4f0-bdfe9c6cc5ed",
   "metadata": {},
   "source": [
    "# 13.2 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a170f953-d7a3-4959-bafe-a8f20a030ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chars: org.apache.spark.rdd.RDD[Char] = MapPartitionsRDD[20] at flatMap at <console>:26\n",
       "KVcharacters: org.apache.spark.rdd.RDD[(Char, Int)] = MapPartitionsRDD[21] at map at <console>:27\n",
       "maxFunc: (left: Int, right: Int)Int\n",
       "addFunc: (left: Int, right: Int)Int\n",
       "nums: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[22] at parallelize at <console>:32\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val chars = words.flatMap(word => word.toLowerCase.toSeq)\n",
    "val KVcharacters = chars.map(letter => (letter,1))\n",
    "\n",
    "def maxFunc(left:Int, right:Int) = math.max(left, right)\n",
    "def addFunc(left:Int, right:Int) = left+right\n",
    "\n",
    "val nums = sc.parallelize(1 to 30, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3c8e6d-779d-4b6a-ad31-c2a6aa6156da",
   "metadata": {},
   "source": [
    "## 13.2.1 countByKey\n",
    "- 각 키의 아이템 수를 구하고 로컬 맵으로 결과를 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94dea420-6637-44cc-bef3-9c624918d3da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timeout: Long = 1000\n",
       "confidence: Double = 0.95\n",
       "res9: org.apache.spark.partial.PartialResult[scala.collection.Map[Char,org.apache.spark.partial.BoundedDouble]] = (final: Map(e -> [7.000, 7.000], s -> [4.000, 4.000], n -> [2.000, 2.000], t -> [3.000, 3.000], u -> [1.000, 1.000], f -> [1.000, 1.000], a -> [4.000, 4.000], m -> [2.000, 2.000], i -> [7.000, 7.000], v -> [1.000, 1.000], b -> [1.000, 1.000], g -> [3.000, 3.000], l -> [1.000, 1.000], p -> [3.000, 3.000], c -> [1.000, 1.000], h -> [1.000, 1.000], r -> [2.000, 2.000], : -> [1.000, 1.000], k -> [1.000, 1.000], o -> [1.000, 1.000], d -> [4.000, 4.000]))\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val timeout = 1000L //밀리세컨드 단위\n",
    "val confidence = 0.95\n",
    "\n",
    "KVcharacters.countByKey()\n",
    "KVcharacters.countByKeyApprox(timeout, confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7fd6f8-2e84-4f44-a95f-87c2db6834c9",
   "metadata": {},
   "source": [
    "## 13.2.2 집계 연산 구현 방식 이해하기\n",
    "- 구현 방식은 잡의 안정성을 위해 매우 중요\n",
    "- groupBy와 reduce 함수의 비교를 통해 이해해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e24f15-31b6-46c8-a0eb-4a70812411c9",
   "metadata": {},
   "source": [
    "## groupByKey\n",
    "- 각 키의 총 레코드 수를 구하는 경우 groupByKey의 결과로 만들어진 그룹에 map 연산을 수행하는 방식이 가장 좋아 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aae713a1-3c04-4f20-91c4-5d3742f30045",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "java.lang.InternalError",
     "evalue": " java.lang.IllegalAccessException: final field has no write access: $Lambda$3252/0x0000000801c94e40.arg$1/putField, from class java.lang.Object (module java.base)",
     "output_type": "error",
     "traceback": [
      "java.lang.InternalError: java.lang.IllegalAccessException: final field has no write access: $Lambda$3252/0x0000000801c94e40.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:167)",
      "  at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:145)",
      "  at java.base/java.lang.reflect.Field.acquireOverrideFieldAccessor(Field.java:1184)",
      "  at java.base/java.lang.reflect.Field.getOverrideFieldAccessor(Field.java:1153)",
      "  at java.base/java.lang.reflect.Field.set(Field.java:820)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:406)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2501)",
      "  at org.apache.spark.rdd.RDD.$anonfun$map$1(RDD.scala:414)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.RDD.map(RDD.scala:413)",
      "  ... 34 elided",
      "Caused by: java.lang.IllegalAccessException: final field has no write access: $Lambda$3252/0x0000000801c94e40.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectField(MethodHandles.java:3511)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectSetter(MethodHandles.java:3502)",
      "  at java.base/java.lang.invoke.MethodHandleImpl$1.unreflectField(MethodHandleImpl.java:1630)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:145)",
      "  ... 46 more",
      ""
     ]
    }
   ],
   "source": [
    "KVcharacters.groupByKey().map(row => (row._1, row._2.reduce(addFunc))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227c7573-6bd5-4d2a-80ed-c6c82d246405",
   "metadata": {},
   "source": [
    "- 모든 익스큐터에서 함수를 적용하기 전에 해당 키와 관련된 모든 값을 메모리로 읽어 들여야 함\n",
    "- 심각하게 치우쳐진 키가 있다면 일부 파티션이 엄청난 양의 값을 가질 수 있으므로 OutOfMemoryError가 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eed2b1-776d-4029-944b-91beae8b56de",
   "metadata": {},
   "source": [
    "## reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4b8634c-d2e7-42ec-8182-1b401fd2d5f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "java.lang.InternalError",
     "evalue": " java.lang.IllegalAccessException: final field has no write access: $Lambda$3282/0x0000000801ca5828.arg$1/putField, from class java.lang.Object (module java.base)",
     "output_type": "error",
     "traceback": [
      "java.lang.InternalError: java.lang.IllegalAccessException: final field has no write access: $Lambda$3282/0x0000000801ca5828.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:167)",
      "  at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:145)",
      "  at java.base/java.lang.reflect.Field.acquireOverrideFieldAccessor(Field.java:1184)",
      "  at java.base/java.lang.reflect.Field.getOverrideFieldAccessor(Field.java:1153)",
      "  at java.base/java.lang.reflect.Field.set(Field.java:820)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:406)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2501)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$combineByKeyWithClassTag$1(PairRDDFunctions.scala:88)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.PairRDDFunctions.combineByKeyWithClassTag(PairRDDFunctions.scala:76)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$reduceByKey$1(PairRDDFunctions.scala:304)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:304)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$reduceByKey$4(PairRDDFunctions.scala:323)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:323)",
      "  ... 34 elided",
      "Caused by: java.lang.IllegalAccessException: final field has no write access: $Lambda$3282/0x0000000801ca5828.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectField(MethodHandles.java:3511)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectSetter(MethodHandles.java:3502)",
      "  at java.base/java.lang.invoke.MethodHandleImpl$1.unreflectField(MethodHandleImpl.java:1630)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:145)",
      "  ... 56 more",
      ""
     ]
    }
   ],
   "source": [
    "KVcharacters.reduceByKey(addFunc).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d09a232-2207-420c-b287-30c7dbe45e95",
   "metadata": {},
   "source": [
    "- flatMap을 동일하게 수행한 다음 map 연산을 사용해 각 문자의 인스턴스를 1로 매핑\n",
    "- 그런 다음 결괏값을 배열에 모을 수 있도록 합계 함수와 함께 reduceByKey 메서드를 수행\n",
    "- 이러한 구현 방식은 각 파티션에서 리듀스 작업을 수행하기 때문에 훨씬 안정적이며 모든 값을 메모리에 유지하지 않아도 됨\n",
    "- 또한 최종 리듀스 과정을 제외한 모든 작업은 개별 워커에서 처리하기 때문에 연산 중 셔플이 발생하지 않음\n",
    "- 안정성과 연산 수행 속도가 크게 향상됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed82a20-e760-42fe-b085-089860419591",
   "metadata": {},
   "source": [
    "## 13.2.3 기타 집계 메서드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e7136-dfc5-4474-8636-352a766b9097",
   "metadata": {},
   "source": [
    "### aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f1a46b1-a367-4d69-af40-5461b3ed26ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "java.lang.InternalError",
     "evalue": " java.lang.IllegalAccessException: final field has no write access: $Lambda$3325/0x0000000801cbd448.arg$1/putField, from class java.lang.Object (module java.base)",
     "output_type": "error",
     "traceback": [
      "java.lang.InternalError: java.lang.IllegalAccessException: final field has no write access: $Lambda$3325/0x0000000801cbd448.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:167)",
      "  at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:145)",
      "  at java.base/java.lang.reflect.Field.acquireOverrideFieldAccessor(Field.java:1184)",
      "  at java.base/java.lang.reflect.Field.getOverrideFieldAccessor(Field.java:1153)",
      "  at java.base/java.lang.reflect.Field.set(Field.java:820)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:406)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2501)",
      "  at org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1196)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)",
      "  ... 34 elided",
      "Caused by: java.lang.IllegalAccessException: final field has no write access: $Lambda$3325/0x0000000801cbd448.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectField(MethodHandles.java:3511)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectSetter(MethodHandles.java:3502)",
      "  at java.base/java.lang.invoke.MethodHandleImpl$1.unreflectField(MethodHandleImpl.java:1630)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:145)",
      "  ... 46 more",
      ""
     ]
    }
   ],
   "source": [
    "nums.aggregate(0)(maxFunc, addFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bda42c17-a9f7-4075-add0-611fb65a854f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "java.lang.InternalError",
     "evalue": " java.lang.IllegalAccessException: final field has no write access: $Lambda$3328/0x0000000801d00420.arg$1/putField, from class java.lang.Object (module java.base)",
     "output_type": "error",
     "traceback": [
      "java.lang.InternalError: java.lang.IllegalAccessException: final field has no write access: $Lambda$3328/0x0000000801d00420.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:167)",
      "  at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:145)",
      "  at java.base/java.lang.reflect.Field.acquireOverrideFieldAccessor(Field.java:1184)",
      "  at java.base/java.lang.reflect.Field.getOverrideFieldAccessor(Field.java:1153)",
      "  at java.base/java.lang.reflect.Field.set(Field.java:820)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:406)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2501)",
      "  at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1233)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)",
      "  at org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)",
      "  ... 34 elided",
      "Caused by: java.lang.IllegalAccessException: final field has no write access: $Lambda$3328/0x0000000801d00420.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectField(MethodHandles.java:3511)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectSetter(MethodHandles.java:3502)",
      "  at java.base/java.lang.invoke.MethodHandleImpl$1.unreflectField(MethodHandleImpl.java:1630)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:145)",
      "  ... 51 more",
      ""
     ]
    }
   ],
   "source": [
    "val depth = 3\n",
    "nums.treeAggregate(0)(maxFunc, addFunc, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0177cd16-d475-4dba-a09a-747af4ebf05a",
   "metadata": {},
   "source": [
    "### aggregateByKey\n",
    "- 키를 기준으로 연산을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9df52192-8401-4797-b9bd-ab498f67972b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "java.lang.InternalError",
     "evalue": " java.lang.IllegalAccessException: final field has no write access: $Lambda$3495/0x0000000801d4c000.arg$1/putField, from class java.lang.Object (module java.base)",
     "output_type": "error",
     "traceback": [
      "java.lang.InternalError: java.lang.IllegalAccessException: final field has no write access: $Lambda$3495/0x0000000801d4c000.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:167)",
      "  at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:145)",
      "  at java.base/java.lang.reflect.Field.acquireOverrideFieldAccessor(Field.java:1184)",
      "  at java.base/java.lang.reflect.Field.getOverrideFieldAccessor(Field.java:1153)",
      "  at java.base/java.lang.reflect.Field.set(Field.java:820)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:406)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2501)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$aggregateByKey$1(PairRDDFunctions.scala:168)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.PairRDDFunctions.aggregateByKey(PairRDDFunctions.scala:158)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$aggregateByKey$5(PairRDDFunctions.scala:198)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.PairRDDFunctions.aggregateByKey(PairRDDFunctions.scala:198)",
      "  ... 34 elided",
      "Caused by: java.lang.IllegalAccessException: final field has no write access: $Lambda$3495/0x0000000801d4c000.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectField(MethodHandles.java:3511)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectSetter(MethodHandles.java:3502)",
      "  at java.base/java.lang.invoke.MethodHandleImpl$1.unreflectField(MethodHandleImpl.java:1630)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:145)",
      "  ... 51 more",
      ""
     ]
    }
   ],
   "source": [
    "KVcharacters.aggregateByKey(0)(addFunc, maxFunc).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c562b-a265-4913-8880-a57712047c07",
   "metadata": {},
   "source": [
    "### combineByKey\n",
    "- 집계 함수 대신 컴바이너를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "996c5951-e617-484a-a08e-7e06e2744c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "valToCombiner: Int => List[Int] = $Lambda$3504/0x0000000801c56000@65c99158\n",
       "mergeValuesFunc: (List[Int], Int) => List[Int] = $Lambda$3505/0x0000000801d50000@570e26ad\n",
       "mergeCombinerFunc: (List[Int], List[Int]) => List[Int] = $Lambda$3506/0x0000000801d505a0@1c32a3f5\n",
       "outputPartitions: Int = 6\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val valToCombiner = (value:Int) => List(value)\n",
    "val mergeValuesFunc = (vals:List[Int], valToAppend:Int) => valToAppend :: vals\n",
    "val mergeCombinerFunc = (vals1:List[Int], vals2:List[Int]) => vals1 ::: vals2\n",
    "\n",
    "val outputPartitions = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "054e7510-18a7-490a-b621-179d9fbb7f74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Array[(Char, List[Int])] = Array((f,List(1)), (r,List(1, 1)), (l,List(1)), (s,List(1, 1, 1, 1)), (a,List(1, 1, 1, 1)), (g,List(1, 1, 1)), (m,List(1, 1)), (t,List(1, 1, 1)), (b,List(1)), (h,List(1)), (n,List(1, 1)), (i,List(1, 1, 1, 1, 1, 1, 1)), (u,List(1)), (o,List(1)), (c,List(1)), (d,List(1, 1, 1, 1)), (p,List(1, 1, 1)), (v,List(1)), (:,List(1)), (e,List(1, 1, 1, 1, 1, 1, 1)), (k,List(1)))\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KVcharacters.combineByKey(valToCombiner,mergeValuesFunc,mergeCombinerFunc,outputPartitions).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3801b12-3d9b-47f0-8a1f-f068d6ff0352",
   "metadata": {},
   "source": [
    "### foldByKey\n",
    "- 결합 함수와 항등원인 '제로값'을 이용해 각 키의 값을 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71b25318-826e-4842-a484-4fabd24252c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "java.lang.InternalError",
     "evalue": " java.lang.IllegalAccessException: final field has no write access: $Lambda$3529/0x0000000801d539b8.arg$1/putField, from class java.lang.Object (module java.base)",
     "output_type": "error",
     "traceback": [
      "java.lang.InternalError: java.lang.IllegalAccessException: final field has no write access: $Lambda$3529/0x0000000801d539b8.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:167)",
      "  at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:145)",
      "  at java.base/java.lang.reflect.Field.acquireOverrideFieldAccessor(Field.java:1184)",
      "  at java.base/java.lang.reflect.Field.getOverrideFieldAccessor(Field.java:1153)",
      "  at java.base/java.lang.reflect.Field.set(Field.java:820)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:406)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2501)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$foldByKey$1(PairRDDFunctions.scala:218)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.PairRDDFunctions.foldByKey(PairRDDFunctions.scala:208)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$foldByKey$5(PairRDDFunctions.scala:238)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.PairRDDFunctions.foldByKey(PairRDDFunctions.scala:238)",
      "  ... 34 elided",
      "Caused by: java.lang.IllegalAccessException: final field has no write access: $Lambda$3529/0x0000000801d539b8.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectField(MethodHandles.java:3511)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectSetter(MethodHandles.java:3502)",
      "  at java.base/java.lang.invoke.MethodHandleImpl$1.unreflectField(MethodHandleImpl.java:1630)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:145)",
      "  ... 51 more",
      ""
     ]
    }
   ],
   "source": [
    "KVcharacters.foldByKey(0)(addFunc).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54357c5b-34b3-4334-bc82-1a2ee64ba94e",
   "metadata": {},
   "source": [
    "# 13.3 cogroup\n",
    "- 스칼라를 사용하는 경우 최대 3개, 파이썬을 사용하는 경우 최대 2개의 키-값 형태의 RDD를 그룹화할 수 있으며 각 키를 기준으로 값을 결합\n",
    "- 즉, RDD에 대한 그룹 기반의 조인 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ead75fca-affa-45cd-96c5-e9d24f3be054",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.util.Random\n",
       "distinctChars: org.apache.spark.rdd.RDD[Char] = MapPartitionsRDD[31] at distinct at <console>:28\n",
       "charRDD: org.apache.spark.rdd.RDD[(Char, Double)] = MapPartitionsRDD[32] at map at <console>:29\n",
       "charRDD2: org.apache.spark.rdd.RDD[(Char, Double)] = MapPartitionsRDD[33] at map at <console>:30\n",
       "charRDD3: org.apache.spark.rdd.RDD[(Char, Double)] = MapPartitionsRDD[34] at map at <console>:31\n",
       "res17: Array[(Char, (Iterable[Double], Iterable[Double], Iterable[Double]))] = Array((d,(CompactBuffer(0.3762916423043762),CompactBuffer(0.9651337597778552),CompactBuffer(0.6600202097004217))), (p,(CompactBuffer(0.48745951298651935),CompactBuffer(0.4990312446157963),CompactBuffer(0.7153875797973754))), (t,(CompactBuffer(0.737476270447212),CompactBuffer(0.833937437349693),CompactBuff...\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Random\n",
    "\n",
    "val distinctChars = words.flatMap(word => word.toLowerCase.toSeq).distinct\n",
    "val charRDD = distinctChars.map(c => (c, new Random().nextDouble()))\n",
    "val charRDD2 = distinctChars.map(c => (c, new Random().nextDouble()))\n",
    "val charRDD3 = distinctChars.map(c => (c, new Random().nextDouble()))\n",
    "\n",
    "charRDD.cogroup(charRDD2, charRDD3).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be2bb8-47bd-48b3-ae14-61f2b85f9954",
   "metadata": {},
   "source": [
    "# 13.4 조인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e411b7-a584-42db-81dd-5445d10202c0",
   "metadata": {},
   "source": [
    "## 13.4.1 내부 조인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a00c433-979b-468c-b274-56f4cb94adc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keyedChars: org.apache.spark.rdd.RDD[(Char, Double)] = MapPartitionsRDD[37] at map at <console>:29\n",
       "outputPartitions: Int = 10\n",
       "res18: Long = 51\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val keyedChars = distinctChars.map(c => (c, new Random().nextDouble()))\n",
    "val outputPartitions = 10\n",
    "\n",
    "KVcharacters.join(keyedChars).count()\n",
    "KVcharacters.join(keyedChars, outputPartitions).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17089ab9-0066-471e-bbbb-7e321a3285b3",
   "metadata": {},
   "source": [
    "## 13.4.2 zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbfe873c-4107-41bf-ae88-83dbc54efc76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numRange: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[44] at parallelize at <console>:28\n",
       "res19: Array[(String, Int)] = Array((Spark,0), (The,1), (Definitive,2), (Guide,3), (:,4), (Big,5), (Data,6), (Processing,7), (Made,8), (Simple,9))\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numRange = sc.parallelize(0 to 9, 2)\n",
    "words.zip(numRange).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47761ee-80fd-4d4c-bc18-5074fdcd54b8",
   "metadata": {},
   "source": [
    "# 13.5 파티션 제어하기\n",
    "- RDD를 사용하면 데이터가 클러스터 전체에 물리적으로 정확히 분산되는 방식을 정의할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff32d617-0007-4500-b1cf-9213628ac8dd",
   "metadata": {},
   "source": [
    "## 13.5.1 coalesce\n",
    "- 파티션을 재분배할 때 발생하는 데이터 셔플을 방지하기 위해 동일한 워커에 존재하는 파티션을 합치는 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afddc5c1-68dd-4120-9bfd-569d857759d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res20: Int = 1\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.coalesce(1).getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9263c648-d2d1-42b7-a2f7-c7510cfd34d1",
   "metadata": {},
   "source": [
    "## 13.5.2 repartition\n",
    "- 처리 시 노드 간의 셔플이 발생할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a063a2c-93de-485c-8f3f-b42b0ea13286",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[50] at repartition at <console>:29\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.repartition(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd26c195-3910-4f21-8d76-0cea78c1bf7d",
   "metadata": {},
   "source": [
    "## 13.5.3 repartitionAndSortWithinPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679dc768-a7b5-413e-8341-61af7a41cf70",
   "metadata": {},
   "source": [
    "## 13.5.4 사용자 정의 파티셔닝\n",
    "- RDD를 사용하는 가장 큰 이유 중 하나\n",
    "- 잡이 성공적으로 동작되는지 여부에 상당한 영향을 미침\n",
    "- 대표적인 예 : 페이지랭크\n",
    "    - 사용자 정의 파티셔닝을 이용해 클러스터의 배치 구조를 제어하고 셔플을 회피\n",
    "- 데이터 치우침 같은 문제를 피하고자 클러스터 전체에 걸쳐 데이터를 균등하게 분배\n",
    "- 구조적 API로 RDD를 얻고 사용자 정의 파티셔너를 적용한 다음 다시 DataFrame이나 Dataset으로 변환해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b60b6e9c-b825-44af-b4f6-70881efc9750",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[66] at rdd at <console>:27\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"./sample_data/retail-data/all/\")\n",
    "val rdd = df.coalesce(10).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f9f13b8-8773-45a3-800d-ae114b906e13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c368d90-5c50-479d-973a-27acfd3926ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17850\n",
      "17850\n",
      "17850\n",
      "17850\n",
      "17850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\n",
       "keyedRDD: org.apache.spark.rdd.RDD[(Double, org.apache.spark.sql.Row)] = MapPartitionsRDD[68] at keyBy at <console>:30\n",
       "res23: Array[(Double, org.apache.spark.sql.Row)] = Array((15100.0,[536374,21258,VICTORIAN SEWING BOX LARGE,32,12/1/2010 9:09,10.95,15100,United Kingdom]), (16250.0,[536388,21754,HOME BUILDING BLOCK WORD,3,12/1/2010 9:59,5.95,16250,United Kingdom]), (16250.0,[536388,21755,LOVE BUILDING BLOCK WORD,3,12/1/2010 9:59,5.95,16250,United Kingdom]), (16250.0,[536388,21523,DOORMAT FANCY FONT HOME SWEET HOME,2,12/1/2010 9:59,7.95,16250,United Kingdom]), (16250.0,[536388,21363,HOME SMALL WOOD LETTERS,3,12/1/2010 9:59,4.95,16250,United Kingdom]), (16250.0,[536388,21411,GINGHAM HEART  DOORSTOP RED,3,12/1/2010 9:59,4.25,16250,United Kingdom]), (...\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "rdd.map(r => r(6)).take(5).foreach(println)\n",
    "val keyedRDD = rdd.keyBy(row => row(6).asInstanceOf[Int].toDouble)\n",
    "\n",
    "keyedRDD.partitionBy(new HashPartitioner(10)).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "497ff4f9-bd28-49b3-a772-92883e507718",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.Partitioner\n",
       "defined class DomainPartitioner\n",
       "res24: Array[Int] = Array(2, 4299, 4308)\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.Partitioner\n",
    "\n",
    "class DomainPartitioner extends Partitioner {\n",
    "    def numPartitions = 3\n",
    "    def getPartition(key: Any): Int = {\n",
    "        val customerId = key.asInstanceOf[Double].toInt\n",
    "        \n",
    "        if(customerId == 17850.0 || customerId == 12583.0) {\n",
    "            return 0\n",
    "        } else {\n",
    "            return new java.util.Random().nextInt(2) + 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "keyedRDD.partitionBy(new DomainPartitioner).map(_._1).glom().map(_.toSet.toSeq.length).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9fb762-b44a-468a-a67d-81d92b9236c0",
   "metadata": {},
   "source": [
    "# 13.6 사용자 정의 직렬화\n",
    "- Kyro 직렬화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20f2738e-69a2-4565-acc5-34cd9f7cbcb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "java.lang.InternalError",
     "evalue": " java.lang.IllegalAccessException: final field has no write access: $Lambda$4907/0x0000000802233448.arg$1/putField, from class java.lang.Object (module java.base)",
     "output_type": "error",
     "traceback": [
      "java.lang.InternalError: java.lang.IllegalAccessException: final field has no write access: $Lambda$4907/0x0000000802233448.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:167)",
      "  at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:145)",
      "  at java.base/java.lang.reflect.Field.acquireOverrideFieldAccessor(Field.java:1184)",
      "  at java.base/java.lang.reflect.Field.getOverrideFieldAccessor(Field.java:1153)",
      "  at java.base/java.lang.reflect.Field.set(Field.java:820)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:406)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2501)",
      "  at org.apache.spark.rdd.RDD.$anonfun$map$1(RDD.scala:414)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.RDD.map(RDD.scala:413)",
      "  ... 35 elided",
      "Caused by: java.lang.IllegalAccessException: final field has no write access: $Lambda$4907/0x0000000802233448.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectField(MethodHandles.java:3511)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectSetter(MethodHandles.java:3502)",
      "  at java.base/java.lang.invoke.MethodHandleImpl$1.unreflectField(MethodHandleImpl.java:1630)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:145)",
      "  ... 47 more",
      ""
     ]
    }
   ],
   "source": [
    "class SomeClass extends Serializable {\n",
    "    var someValue = 0\n",
    "    def setSomeValue(i:Int) = {\n",
    "        someValue = i\n",
    "        this\n",
    "    }\n",
    "}\n",
    "\n",
    "sc.parallelize(1 to 10).map(num => new SomeClass().setSomeValue(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61806d54-4090-4d5f-9e79-82e1b6015b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

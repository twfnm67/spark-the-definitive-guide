{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aff390e0-05fa-4f0b-b34e-46567a973bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://172.30.53.50:4040\n",
       "SparkContext available as 'sc' (version = 3.3.2, master = local[*], app id = local-1679032734263)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@1a7ec20f\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e713d638-c67a-4ec9-91e4-986de015e614",
   "metadata": {},
   "source": [
    "# 8.1 조인 표현식\n",
    "- 스파크는 왼쪽과 오른쪽 데이터셋에 있는 하나 이상의 키값을 비교하고 왼쪽 데이터셋과 오른쪽 데이터셋의 결합 여부를 결정하는 조인 표현식의 평가 결과에 따라 두 개의 데이터셋을 조인\n",
    "- 가장 많이 사용하는 조인 표현식 : 왼쪽과 오른쪽 데이터셋에 지정된 키가 동일한지 비교하는 동등 조인(equi-join)\n",
    "    - 키가 일치하면 스파크는 왼쪽과 오른쪽 데이터셋을 결합\n",
    "    - 일치하지 않으면 데이터셋을 결합하지 않음\n",
    "- 스파크는 일치하는 키가 없는 로우는 조인에 포함시키지 않음\n",
    "- 동등 조인 뿐만 아니라 더 복잡한 조인 정책도 지원, 복합 데이터 타입을 조인에 사용할 수도 있음\n",
    "    - e.g. 배열 타입의 키에 조인할 키가 존재하는지 확인해 조인을 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd955c2-c49d-4058-b067-ae49f69ee073",
   "metadata": {},
   "source": [
    "# 8.2 조인 타입\n",
    "- 조인 표현식은 두 로우의 조인 여부를 결정\n",
    "- 조인 타입은 결과 데이터셋에 어떤 데이터가 있어야 하는지 결정\n",
    "    - 내부 조인(inner join) : 왼쪽고 오른쪽 데이터셋에 키가 있는 로우를 유지\n",
    "    - 외부 조인(outer join) : 왼쪽이나 오른쪽 데이터셋에 키가 있는 로우를 유지\n",
    "    - 왼쪽 외부 조인(left outer join) : 왼쪽 데이터셋에 키가 있는 로우를 유지\n",
    "    - 오른쪽 외부 조인(right outer join) : 오른쪽 데이터셋에 키가 있는 로우를 유지\n",
    "    - 왼쪽 세미 조인(left semi join) : 왼쪽 데이터셋의 키가 오른쪽 데이터셋에 있는 경우에는 키가 일치하는 왼쪽 데이터셋만 유지\n",
    "    - 왼쪽 안티 조인(left anti join) : 왼쪽 데이터셋의 키가 오른쪽 데이터셋에 없는 경우에는 키가 일치하지 않는 왼쪽 데이터셋만 유지\n",
    "    - 자연 조인(natural join) : 두 데이터셋에서 동일한 이름을 가진 커럼을 암시적(implicit)으로 결합하는 조인\n",
    "    - 교차 조인(cross join) 또는 카테시안 조인(Cartesion join) : 왼쪽 데이터셋의 모든 로우와 오른쪽 데이터셋의 모든 로우를 조합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a958d604-0912-47d4-b741-66296533250d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person: org.apache.spark.sql.DataFrame = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val person = Seq(\n",
    "    (0, \"Bill Chambers\", 0, Seq(100)),\n",
    "    (1, \"Matei Zaharia\", 1, Seq(500, 250, 100)),\n",
    "    (2, \"Michael Armbrust\", 1, Seq(250, 100)))\n",
    ".toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6cefeab-6cfa-4aef-99d0-2ba21b05958c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graduateProgram: org.apache.spark.sql.DataFrame = [id: int, degree: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graduateProgram = Seq(\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkelye\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkely\"),\n",
    "    (1, \"Ph.D\", \"EECS\", \"UC Berkely\"))\n",
    ".toDF(\"id\", \"degree\", \"department\", \"school\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deefcc18-f764-44cf-bb3b-518cd455e112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkStatus: org.apache.spark.sql.DataFrame = [id: int, status: string]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkStatus = Seq(\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\"))\n",
    ".toDF(\"id\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b30b7373-7a8d-453c-93f1-dd2a3029432c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "sparkStatus.createOrReplaceTempView(\"sparkStatus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a66e0-b6cd-48ef-ab32-b6092e4a5359",
   "metadata": {},
   "source": [
    "# 8.3 내부 조인\n",
    "- 내부 조인은 DataFrame이나 테이블에 존재하는 키를 평가하고 참(true)으로 평가되는 로우만 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b236bf57-8894-43cf-800c-25e5e354fb5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinExpression: org.apache.spark.sql.Column = (graduate_program = id)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joinExpression = person.col(\"graduate_program\") === graduateProgram.col(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e6c8a-0935-4187-8118-4066ebffdc15",
   "metadata": {},
   "source": [
    "- 두 DataFrame 모두에 키가 존재하지 않으면 결과 DataFrame에서 볼 수 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08c7310a-5ff7-4afd-b471-ecfb15c8592e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wrongJoinExpression: org.apache.spark.sql.Column = (name = school)\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wrongJoinExpression = person.col(\"name\") === graduateProgram.col(\"school\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c36b1588-9431-477b-a817-ba60cb89d104",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkelye|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|   Ph.D|                EECS| UC Berkely|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|   Ph.D|                EECS| UC Berkely|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(graduateProgram, joinExpression).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e40c73-7e14-4302-9822-c22633cdcdc5",
   "metadata": {},
   "source": [
    "# 8.4 외부 조인\n",
    "- 외부 조인은 DataFrame이나 테이블에 존재하는 키를 평가하여 참이나 거짓으로 평가한 로우를 포함(그리고 조인)함\n",
    "- 왼쪽이나 오른쪽 DataFrame에 일치하는 로우가 없다면 스파크는 해당 위치에 null을 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbc065dc-108e-40e6-b273-8083befb94d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinType: String = outer\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var joinType = \"outer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b69ae43b-d794-4d5c-96f9-d617e31a72f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkelye|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|   Ph.D|                EECS| UC Berkely|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|   Ph.D|                EECS| UC Berkely|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS| UC Berkely|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567808a-53ad-4483-909e-069e554d9be9",
   "metadata": {},
   "source": [
    "# 8.5 왼쪽 외부 조인\n",
    "- 왼쪽 외부 조인은 DataFrame이나 테이블에 존재하는 키를 평가\n",
    "- 왼쪽 DataFrame의 모든 로우와 왼쪽 DataFrame과 일치하는 오른쪽 DataFrame의 로우를 함께 포함\n",
    "- 오른쪽 DataFrame에 일치하는 로우가 없다면 스파크는 해당 위치에 null을 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4f33d67-eabd-4837-8aba-2c128e1edf8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school|  id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkelye|   0|   Bill Chambers|               0|          [100]|\n",
      "|  2|Masters|                EECS| UC Berkely|null|            null|            null|           null|\n",
      "|  1|   Ph.D|                EECS| UC Berkely|   2|Michael Armbrust|               1|     [250, 100]|\n",
      "|  1|   Ph.D|                EECS| UC Berkely|   1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinType: String = left_outer\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinType = \"left_outer\"\n",
    "graduateProgram.join(person,joinExpression,joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93abc77f-8c98-464c-a14c-20e01f26fbb4",
   "metadata": {},
   "source": [
    "# 8.6 오른쪽 외부 조인\n",
    "- 오른쪽 외부 조인은 DataFrame이나 테이블에 존재하는 키를 평가\n",
    "- 오른쪽 DataFrame의 모든 로우와 오른쪽 DataFrame과 일치하는 왼쪽 DataFrame의 로우를 함께 포함\n",
    "- 왼쪽 DataFrame에 일치하는 로우가 없다면 스파크는 해당 위치에 null을 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aefe27e6-2cb7-48c3-974b-b08028280550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkelye|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS| UC Berkely|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|   Ph.D|                EECS| UC Berkely|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|   Ph.D|                EECS| UC Berkely|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinType: String = right_outer\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinType = \"right_outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ac27d-8959-4dda-adf2-7c5fa9dee058",
   "metadata": {},
   "source": [
    "# 8.7 왼쪽 세미 조인\n",
    "- 세미 조인은 오른쪽 DataFrame의 어떤 값도 포함하지 않기 때문에 다른 조인 타입과는 약간 다름\n",
    "- 단지 두 번재 DataFrame은 값이 존재하는지 확인하기 위해 값만 비교하는 용도로 사용\n",
    "- 만약 값이 존재한다면 왼쪽 DataFrame에 중복 키가 존재하더라도 해당 로우는 결과에 포함됨\n",
    "- 왼쪽 세미 조인은 기존 조인 기능과는 달리 DataFrame의 필터 정도로 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1441edb8-2c9f-4415-a07f-c7ef5638c291",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkelye|\n",
      "|  1|   Ph.D|                EECS| UC Berkely|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinType: String = left_semi\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinType = \"left_semi\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee117c43-c12c-4b19-b5cf-0ef8cc64d236",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradProgram2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, degree: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gradProgram2 = graduateProgram.union(Seq(\n",
    "    (0, \"Masters\", \"Duplicated row\", \"Duplicated School\")).toDF())\n",
    "gradProgram2.createOrReplaceTempView(\"gradProgram2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe41adb-e203-49a8-a60b-c03648118979",
   "metadata": {},
   "source": [
    "# 8.8 왼쪽 안티 조인\n",
    "- 왼쪽 세미 조인의 반대 개념\n",
    "- 왼쪽 세미 조인처럼 오른쪽 DataFrame의 어떤 값도 포함하지 않음\n",
    "- 단지 두 번째 DataFrame은 값이 존재하는지 확인하기 위해 값만 비교하는 용도로 사용\n",
    "- 하지만 두 번째 DataFrame에 존재하는 값을 유지하는 대신 두 번재 DataFrame에서 관련된 키를 찾을 수 없는 로우만 결과에 포함\n",
    "- 안티 조인은 SQL의 NOT IN과 같은 스타일의 필터로 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6d43e09-2490-445a-bf5b-691aed60b59a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+----------+\n",
      "| id| degree|department|    school|\n",
      "+---+-------+----------+----------+\n",
      "|  2|Masters|      EECS|UC Berkely|\n",
      "+---+-------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinType: String = left_anti\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinType = \"left_anti\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d837f9-f62f-42bb-b3e2-50ca0b97baa5",
   "metadata": {},
   "source": [
    "# 8.9 자연 조인\n",
    "- 조인하려는 컬럼을 암시적으로 추정\n",
    "    - 즉, 일치하는 컬럼을 찾고 그 결과를 반환\n",
    "- 왼쪽과 오른쪽 그리고 외부 자연 조인을 사용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa65fe-b5d4-49e9-8d88-d3da23059616",
   "metadata": {},
   "source": [
    "# 8.10 교차 조인(카테시안 조인)\n",
    "- 조건절을 기술하지 않은 내부 조인\n",
    "- 왼쪽 DataFrame의 모든 로우를 오른쪽 DataFrame의 모든 로우와 결합\n",
    "    - 엄청난 수를 가진 DataFrame이 생성될 수 있음\n",
    "    - 따라서 반드시 키워드를 이용해 교차 조인을 수행한다는 것을 명시적으로 선언해야 함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80a9f2dd-40b1-40ef-ad46-f39769079f40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school| id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkelye|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|   Ph.D|                EECS| UC Berkely|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "|  1|   Ph.D|                EECS| UC Berkely|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinType: String = cross\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinType = \"cross\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa49cfe-39aa-438f-91a2-72aa96762368",
   "metadata": {},
   "source": [
    "# 8.11 조인 사용 시 문제점\n",
    "## 8.11.1 복합 데이터 타입의 조인\n",
    "- 불리언을 반환하는 모든 표현식은 조인 표현식으로 간주할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07d81815-a7b9-47e6-bca8-742cbc81e0d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.expr\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63ce4bbb-c306-4ea3-9e4b-9f535090fa90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|personId|            name|graduate_program|   spark_status| id|        status|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|       0|   Bill Chambers|               0|          [100]|100|   Contributor|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|500|Vice President|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|250|    PMC Member|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|100|   Contributor|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|250|    PMC Member|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|100|   Contributor|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.withColumnRenamed(\"id\", \"personId\").join(sparkStatus, expr(\"array_contains(spark_status, id)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aafb77-5c44-4e97-80ee-5a32a491b188",
   "metadata": {},
   "source": [
    "## 8.11.2 중복 컬럼명 처리\n",
    "- 조인을 수행할 때 가장 까다로운 것 중 하나는 결과 DataFrame에서 중복된 컬럼명을 다루는 것\n",
    "- DataFrame의 각 컬럼은 스파크 SQL 엔진인 카탈리스트 내에 고유 ID가 있음\n",
    "- 고유 ID는 카탈리스트 내부에서만 사용할 수 있으며 직접 참조할 수 있는 값이 아님\n",
    "- 그러므로 중복된 컬럼명이 존재하는 DataFrame을 사용할 때는 특정 컬럼을 참조하기 매우 어려움\n",
    "- 이런 문제를 일으키는 두 가지 상황\n",
    "    - 조인에 사용할 DataFrame의 특정 키가 동일한 이름을 가지며, 키가 제거되지 않도록 조인 표현식에 명시하는 경우\n",
    "    - 조인 대상이 아닌 두 개의 컬럼이 동일한 이름을 가진 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52a5942d-395a-4ff0-9d0d-9664ca2c65d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradProgramDupe: org.apache.spark.sql.DataFrame = [graduate_program: int, degree: string ... 2 more fields]\n",
       "joinExpr: org.apache.spark.sql.Column = (graduate_program = graduate_program)\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gradProgramDupe = graduateProgram.withColumnRenamed(\"id\", \"graduate_program\")\n",
    "\n",
    "val joinExpr = gradProgramDupe.col(\"graduate_program\") === person.col(\"graduate_program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a1f756b-b93d-42a4-8d98-5dd4b799f213",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status|graduate_program| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|               0|Masters|School of Informa...|UC Berkelye|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|               1|   Ph.D|                EECS| UC Berkely|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|               1|   Ph.D|                EECS| UC Berkely|\n",
      "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(gradProgramDupe, joinExpr).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e185214-3f44-4c2b-8020-160f9ee19d39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Reference 'graduate_program' is ambiguous, could be: graduate_program, graduate_program.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Reference 'graduate_program' is ambiguous, could be: graduate_program, graduate_program.",
      "  at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:377)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:114)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$resolveExpressionByPlanChildren$1(Analyzer.scala:1807)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$resolveExpression$2(Analyzer.scala:1735)",
      "  at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:60)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.innerResolve$1(Analyzer.scala:1742)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.resolveExpression(Analyzer.scala:1762)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.resolveExpressionByPlanChildren(Analyzer.scala:1813)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$15.$anonfun$applyOrElse$78(Analyzer.scala:1521)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:200)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:200)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:211)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:216)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:216)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:221)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:427)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:221)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$15.applyOrElse(Analyzer.scala:1521)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$15.applyOrElse(Analyzer.scala:1358)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:1358)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:1338)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)",
      "  at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)",
      "  at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)",
      "  at scala.collection.immutable.List.foldLeft(List.scala:91)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)",
      "  at scala.collection.immutable.List.foreach(List.scala:431)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:231)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:227)",
      "  at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:227)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:212)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)",
      "  at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:3887)",
      "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1519)",
      "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1536)",
      "  ... 34 elided",
      ""
     ]
    }
   ],
   "source": [
    "person.join(gradProgramDupe, joinExpr).select(\"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ae3b9-e833-4d7f-aa3a-7cc649d5dfde",
   "metadata": {},
   "source": [
    "### 해결 방법 1: 다른 조인 표현식 사용\n",
    "- 불리언 형태의 조인 표현식을 문자열이나 시퀀스 형태로 바꾸는 것\n",
    "    - 이렇게 하면 조인을 할 때두 컬럼 중 하나가 자동으로 제거됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "076e1e2b-c354-4597-87a6-5e79b049b31f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|graduate_program|\n",
      "+----------------+\n",
      "|               0|\n",
      "|               1|\n",
      "|               1|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(gradProgramDupe, \"graduate_program\").select(\"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c219fb-3557-47ca-84db-87279ae581fa",
   "metadata": {},
   "source": [
    "### 해결 방법 2 : 조인 후 컬럼 제거\n",
    "- 원본 DataFrame을 사용해 컬럼을 참조해야 함\n",
    "- 조인 시 동일한 키 이름을 사용하거나 원본 DataFrame에 동일한 컬럼명이 존재하는 경우에 사용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3be32d1-daab-41a6-acc6-cb450d4e77c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|graduate_program|\n",
      "+----------------+\n",
      "|               0|\n",
      "|               1|\n",
      "|               1|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(gradProgramDupe, joinExpr).drop(person.col(\"graduate_program\")).select(\"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d34faf8-0686-463a-962b-170f627ea681",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|Masters|School of Informa...|UC Berkelye|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|   Ph.D|                EECS| UC Berkely|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|   Ph.D|                EECS| UC Berkely|\n",
      "+---+----------------+----------------+---------------+-------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinExpr: org.apache.spark.sql.Column = (graduate_program = id)\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joinExpr = person.col(\"graduate_program\") === graduateProgram.col(\"id\")\n",
    "person.join(graduateProgram, joinExpr).drop(graduateProgram.col(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc15215-b3f6-43ff-ada7-40db1f13c05d",
   "metadata": {},
   "source": [
    "### 해결 방법 3 : 조인 전 컬럼명 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efa4bbe2-63c6-4583-9d18-326d6f5bc77f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+-------+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status|grad_id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+-------+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|      0|Masters|School of Informa...|UC Berkelye|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|      1|   Ph.D|                EECS| UC Berkely|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|      1|   Ph.D|                EECS| UC Berkely|\n",
      "+---+----------------+----------------+---------------+-------+-------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gradProgram3: org.apache.spark.sql.DataFrame = [grad_id: int, degree: string ... 2 more fields]\n",
       "joinExpr: org.apache.spark.sql.Column = (graduate_program = grad_id)\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gradProgram3 = graduateProgram.withColumnRenamed(\"id\", \"grad_id\")\n",
    "val joinExpr = person.col(\"graduate_program\") === gradProgram3.col(\"grad_id\")\n",
    "person.join(gradProgram3, joinExpr).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69482f-cf77-46af-8619-4cd823259a0e",
   "metadata": {},
   "source": [
    "# 8.12 스파크의 조인 수행 방식\n",
    "- 실행에 필요한 두 가지 핵심 전략\n",
    "    1. 노드간 네트워크 통신 전략\n",
    "    2. 노드별 연산 전략"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d112fdfb-5361-4b1c-98bf-a7c0ce89b4c9",
   "metadata": {},
   "source": [
    "## 8.12.1 네트워크 통신 전략\n",
    "- 스파크는 조인 시 두 가지 클러스터 통신 방식을 활용\n",
    "    - 셔플 조인 : 전체 노드 간 통신을 유발\n",
    "    - 브로드캐스트 조인\n",
    "- 이제부터는 사용자가 스파크에서 사용하는 테이블의 크기가 아주 크거나 아주 작다고 가정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f1c89b-e322-4dc8-8673-a97eebd2bd6e",
   "metadata": {},
   "source": [
    "### 큰 테이블과 큰 테이블 조인\n",
    "- 하나의 큰 테이블을 다른 큰 테이블과 조인하면 셔플 조인이 발생\n",
    "- 셔플 조인은 전체 노드 간 통신 발생. 그리고 조인에 사용한 특정 키나 키 집합을 어떤 노드가 가졌는지에 따라 해당 노드와 데이터를 공유\n",
    "- 이런 통신 방식 때문에 네트워크는 복잡해지고 많은 자원을 사용 (특히 데이터가 잘 나뉘어 있지 않다면 더 심해짐)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb528bd-9b0b-4ff5-9036-79305101c862",
   "metadata": {},
   "source": [
    "### 큰 테이블과 작은 테이블 조인\n",
    "- 테이블이 단일 워커 노드의 메모리 크기에 적합할 정도로 충분히 작은 경우 조인 연산을 최적화할 수 있음\n",
    "- 브로드캐스트 조인이 훨씬 효율적\n",
    "    - 작은 DataFrame을 클러스터의 전체 워커 노드에 복제(조인 프로세스 내내 전체 노드가 통신하는 현상을 방지)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e09d8b7-3041-4ce0-9e93-fc7d5a05168c",
   "metadata": {},
   "source": [
    "### 아주 작은 테이블 사이의 조인\n",
    "- 스파크가 결정하도록 내버려 두는 것이 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5069d4-719a-43e1-a8cb-daa68a6f1a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82a72d02-2b13-420c-b6e9-2b910b9ca051",
   "metadata": {},
   "source": [
    "# 11.1 Dataset을 사용할 시기\n",
    "- DataFrame 기능만으로는 수행할 연산을 표현할 수 없는 경우\n",
    "- 성능 저하를 감수하더라도 타입 안정성(type-safe)을 가진 데이터 타입을 사용하고 싶은 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8eb1a-4a7f-4c9b-a310-ac581a079d13",
   "metadata": {},
   "source": [
    "# 11.2 Dataset 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c208b38e-871b-4b67-bf68-d213be524730",
   "metadata": {},
   "source": [
    "## 11.2.1 자바 : Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6836f90-18aa-4dc0-acdf-6bb4501a8b3e",
   "metadata": {},
   "source": [
    "## 11.2.2 스칼라 : 케이스 클래스\n",
    "- 케이스 클래스\n",
    "    - 불변성\n",
    "    - 패턴 매칭으로 분해 가능\n",
    "    - 참조값 대신 클래스 구조를 기반으로 비교\n",
    "    - 사용하기 쉽고 다루기 편함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f977dc0-27f0-4ecf-904a-b00c690bd39e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5450c6e8\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f39020-e330-4fcd-b872-c0fd9403ce34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Flight\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Flight(DEST_COUNTRY_NAME: String,\n",
    "                  ORIGIN_COUNTRY_NAME: String, count: BigInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96b3cef8-be67-4adb-854e-db532689c3b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flightDF: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n",
       "flights: org.apache.spark.sql.Dataset[Flight] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightDF = spark.read\n",
    ".parquet(\"/Users/choyubin/Downloads/Spark-The-Definitive-Guide-master/data/flight-data/parquet/2010-summary.parquet/\")\n",
    "val flights = flightDF.as[Flight]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a680f993-f723-408a-863e-b6eab23f53e1",
   "metadata": {},
   "source": [
    "# 11.3 액션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "983fea32-3b16-435e-8f00-0ce6d2fa2385",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04f3435f-f73e-4050-a6f0-c458a13f955a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: String = United States\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.first.DEST_COUNTRY_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0acbd4-5fdc-4fce-89df-22a828a021de",
   "metadata": {},
   "source": [
    "# 11.4 트랜스포메이션\n",
    "- DataFrame의 모든 트랜스포메이션은 Dataset에서 사용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dedc7c1-2d6d-41d4-adac-c1c4ab99ac38",
   "metadata": {},
   "source": [
    "## 11.4.1 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bde0d561-6ddd-4004-bd62-ed2927785680",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "originIsDestination: (flight_row: Flight)Boolean\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def originIsDestination(flight_row: Flight): Boolean = {\n",
    "    return flight_row.ORIGIN_COUNTRY_NAME == flight_row.DEST_COUNTRY_NAME\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76cb8036-f578-401f-b34f-004adc8b9360",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[Flight] = Array(Flight(United States,United States,348113))\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.collect().filter(flight_row => originIsDestination(flight_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b37a5-1475-4242-8347-c563f317ae30",
   "metadata": {},
   "source": [
    "## 11.4.2 매핑\n",
    "- 필터링은 단순한 트랜스포메이션\n",
    "- 때로는 특정 값을 다른 값으로 매핑해야할 때가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b27820d7-d7b5-4953-9786-0ae96f0730dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "destinations: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val destinations = flights.map(f => f.DEST_COUNTRY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52bd6efb-fbdc-4d29-952a-3011fedaf279",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/24 20:03:45 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 5)\n",
      "java.lang.ClassCastException: class $line9.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw cannot be cast to class $line9.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw ($line9.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @526c5c28; $line9.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @3874601d)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "23/03/24 20:03:45 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 5) (192.168.0.2 executor driver): java.lang.ClassCastException: class $line9.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw cannot be cast to class $line9.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw ($line9.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @526c5c28; $line9.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @3874601d)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "\n",
      "23/03/24 20:03:45 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (192.168.0.2 executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @526c5c28; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @3874601d)",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (192.168.0.2 executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @526c5c28; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @3874601d)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2863)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:3084)",
      "  ... 34 elided",
      "Caused by: java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @526c5c28; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @3874601d)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:136)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "val localDestinations = destinations.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebdfcf4-13ca-4766-8ab1-1084e65ccfe0",
   "metadata": {},
   "source": [
    "# 11.5 조인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c0c23d8-6b0c-43e9-b001-e741801ba112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class FlightMetadata\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class FlightMetadata(count: BigInt, randomData: BigInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68692108-bac6-4e6b-a2a4-b25b8eae40b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flightsMeta: org.apache.spark.sql.Dataset[FlightMetadata] = [count: bigint, randomData: bigint]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightsMeta = spark.range(500).map(x => (x, scala.util.Random.nextLong))\n",
    ".withColumnRenamed(\"_1\", \"count\").withColumnRenamed(\"_2\", \"randomData\")\n",
    ".as[FlightMetadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "894607e6-6e87-4ca2-a901-2e880962ebac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flights2: org.apache.spark.sql.Dataset[(Flight, FlightMetadata)] = [_1: struct<DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field>, _2: struct<count: bigint, randomData: bigint>]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flights2 = flights\n",
    ".joinWith(flightsMeta, flights.col(\"count\") === flightsMeta.col(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e7d99c5-7ad5-4a06-aba2-3b12bf00c4ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights2.selectExpr(\"_1.DEST_COUNTRY_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81508866-4364-4b16-8c9b-65d54bfc5673",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Array[(Flight, FlightMetadata)] = Array((Flight(United States,Uganda,1),FlightMetadata(1,999576434334245861)), (Flight(United States,French Guiana,1),FlightMetadata(1,999576434334245861)))\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "588d38ef-9a8d-42e4-899c-dffd0082a0e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flights2: org.apache.spark.sql.DataFrame = [count: bigint, DEST_COUNTRY_NAME: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flights2 = flights.join(flightsMeta, Seq(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e160a2d2-e99e-4a98-9802-57185d32b877",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fights2: org.apache.spark.sql.DataFrame = [count: bigint, DEST_COUNTRY_NAME: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fights2 = flights.join(flightsMeta.toDF(), Seq(\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b942d81-b049-431c-94ea-a380c71d84dd",
   "metadata": {},
   "source": [
    "# 11.6 그룹화와 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d525a308-5c5c-4570-ad5a-0016c0d19449",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|count|\n",
      "+--------------------+-----+\n",
      "|            Anguilla|    1|\n",
      "|              Russia|    1|\n",
      "|            Paraguay|    1|\n",
      "|             Senegal|    1|\n",
      "|              Sweden|    1|\n",
      "|            Kiribati|    1|\n",
      "|              Guyana|    1|\n",
      "|         Philippines|    1|\n",
      "|            Malaysia|    1|\n",
      "|           Singapore|    1|\n",
      "|                Fiji|    1|\n",
      "|              Turkey|    1|\n",
      "|             Germany|    1|\n",
      "|         Afghanistan|    1|\n",
      "|              Jordan|    1|\n",
      "|               Palau|    1|\n",
      "|Turks and Caicos ...|    1|\n",
      "|              France|    1|\n",
      "|              Greece|    1|\n",
      "|              Taiwan|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.groupBy(\"DEST_COUNTRY_NAME\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48940f8c-3897-4e6c-9f97-7a89a7823d81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: org.apache.spark.sql.Dataset[(String, Long)] = [key: string, count(1): bigint]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.groupByKey(x => x.DEST_COUNTRY_NAME).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d55ac6-df22-4f59-866b-4517cfd63788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

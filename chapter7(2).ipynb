{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f72d046-dcfc-435a-a7c5-fe3968f4853c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://172.29.64.245:4040\n",
       "SparkContext available as 'sc' (version = 3.3.2, master = local[*], app id = local-1682226123927)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5450c6e8\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef9588fb-2336-47ca-91a9-d2a89ab585c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/23 14:05:20 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//구메 이력 데이터를 사용해 파티션을 훨씬 적은 수로 분할할 수 있도록 리파티셔닝하고 빠르게 접근할 수 있도록 캐싱\n",
    "//파티션 수를 줄이는 이유 : 적은 양의 데이터를 가진 수많은 파일이 존재하기 때문\n",
    "val df = spark.read.format(\"csv\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".load(\"sample_data/retail-data/all/*.csv\")\n",
    ".coalesce(5)\n",
    "\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75172159-b616-451a-86c0-09b4f718be5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Boolean = true\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//데이터셋의 전체 크기를 알아보는 용도 & 메모리에 DataFrame 캐싱 작업을 수행하는 용도\n",
    "df.count() == 541909"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d442b14-dfb5-49a2-9b68-7e535b54540a",
   "metadata": {},
   "source": [
    "# 7.1 집계 함수\n",
    "- 모든 집계는 특별한 경우를 제외한다면 함수를 사용\n",
    "- 집계 함수는 org.apache.spark.sql.functions 패키지에서 찾아볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e33a55-9610-4895-aeb1-70f4d418533f",
   "metadata": {},
   "source": [
    "## 7.1.1 count\n",
    "- count 함수는 두 가지 방식으로 사용할 수 있음\n",
    "    1. count 함수에 특정 컬럼을 지정하는 방식\n",
    "    2. count(\\*)나 count count(1)을 사용하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea26b638-ccbc-4873-9fa6-c4fef3e1cdf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.count\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "df.select(count(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3651f821-35d0-4ab0-8d08-87bd83bfcd98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*) FROM dfTable\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e66d80-ed80-491e-9304-e77cfc3e44bf",
   "metadata": {},
   "source": [
    "## 7.1.2 countDistinct\n",
    "- 전체 레코드 수가 아닌 고유 레코드 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb514dbd-0bfd-4149-a374-13e25f9bdfde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.countDistinct\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.countDistinct\n",
    "\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca7e0b7-b05f-4388-8187-4fc646acc584",
   "metadata": {},
   "source": [
    "## 7.1.3 approx_count_distinct\n",
    "- 대규모 데이터셋을 다루다 보면 정확한 고유 개수가 무의미할 때도 있음\n",
    "- 어느 정도 수준의 정확도를 가지는 근사치만으로도 유의미하다면 해당 함수를 통해 근사치 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc1e3d65-4651-4554-a21d-318b9fc4430b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.approx_count_distinct\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.approx_count_distinct\n",
    "\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b832c296-30af-4e65-9549-5d95a0a6595f",
   "metadata": {},
   "source": [
    "## 7.1.4 first와 last\n",
    "- DataFrame의 첫 번째 값이나 마지막 값을 얻을 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c36ae7f4-e1bc-405f-805b-c640fca96977",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|first(StockCode)|last(StockCode)|\n",
      "+----------------+---------------+\n",
      "|          85123A|          22138|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{first, last}\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{first, last}\n",
    "\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc858c4-3253-4c27-a7e0-3910558181e6",
   "metadata": {},
   "source": [
    "## 7.1.5 min과 max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72e15212-b27b-40ec-82d9-be2ebf6d835e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{min, max}\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{min, max}\n",
    "\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57838c56-4ac5-4561-830a-6b6a9d8847ca",
   "metadata": {},
   "source": [
    "## 7.1.6 sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb4e1fb1-141e-4d5d-9375-fd3fc04ee65a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.sum\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.sum\n",
    "\n",
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3837edc5-540d-4e06-9dd8-0bd4d7d9e3f5",
   "metadata": {},
   "source": [
    "## 7.1.7 sumDistinct\n",
    "- 고윳값 합산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a317bcc1-67b7-4775-83a8-a1e8bdbd18f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.sumDistinct\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.sumDistinct\n",
    "\n",
    "df.select(sumDistinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7afa60-e490-44ab-af36-0ef351df4d4d",
   "metadata": {},
   "source": [
    "## 7.1.8 avg\n",
    "- sum 함수의 결과를 count 함수의 결과로 나누어 평균값을 구할 수 있지만, 스파크의 avg 함수나 mean 함수를 사용하면 평균값을 더 쉽게 구할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f14f25b-2ca0-41a3-af80-6253ad2bf7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n",
      "|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "|                      9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{sum, count, avg, expr}\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{sum, count, avg, expr}\n",
    "\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\n",
    ".selectExpr(\n",
    "    \"total_purchases/total_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f5dd32-5b27-4db8-93cb-d996de8283a9",
   "metadata": {},
   "source": [
    "## 7.1.9 분산과 표준편차\n",
    "\n",
    "- 주변에 데이터가 분포된 정도를 측정하는 방법\n",
    "- 분산은 평균과의 차이를 제곱한 결과의 평균\n",
    "- 표준편차는 분산의 제곱근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bbeaaf1-5e9a-4501-a3a4-6f007af1b771",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|47559.30364660918| 47559.39140929887|  218.08095663447824|   218.08115785023443|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{var_pop, stddev_pop}\n",
       "import org.apache.spark.sql.functions.{var_samp, stddev_samp}\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{var_pop, stddev_pop}\n",
    "import org.apache.spark.sql.functions.{var_samp, stddev_samp}\n",
    "\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n",
    "          stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8a8bba-9ce9-46fa-adfa-381dd4d4f59a",
   "metadata": {},
   "source": [
    "## 7.1.10 비대칭도와 첨도\n",
    "- 비대칭도와 첨도 모두 데이터의 변곡점을 측정하는 방법\n",
    "- 비대칭도는 데이터 평균의 비대칭 정도를 측정, 첨도는 데이터 끝 부분을 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1cbe5e2-59f8-4dc0-b003-22aba075e2a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "| skewness(Quantity)|kurtosis(Quantity)|\n",
      "+-------------------+------------------+\n",
      "|-0.2640755761052702|119768.05495532964|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{skewness, kurtosis}\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{skewness, kurtosis}\n",
    "\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3614cc6a-0365-4227-94ac-21393a5bb82a",
   "metadata": {},
   "source": [
    "## 7.1.11 공분산과 상관관계\n",
    "- 단일컬럼이 아닌 두 컬럼값 사이의 영향도를 비교\n",
    "- cov, corr 함수를 사용해 공분산과 상관관계를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd969e93-e8b8-4489-ac26-6ff2e7ad4e87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085641572E-4|              1052.728054391933|            1052.7260778758289|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{corr, covar_pop, covar_samp}\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{corr, covar_pop, covar_samp}\n",
    "\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"),\n",
    "          covar_pop(\"InvoiceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade5bcd-24c5-425e-a554-0a4d1a6eca54",
   "metadata": {},
   "source": [
    "## 7.1.12 복합 데이터 타입의 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a6c7b0a-61b8-4cb9-b797-18314ff9b77c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{collect_set, collect_list}\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{collect_set, collect_list}\n",
    "\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d6b960-b081-44b6-96e3-f8e9265c2000",
   "metadata": {},
   "source": [
    "# 7.2 그룹화\n",
    "- 단일 컬럼의 데이터를 그룹화하고 해당 그룹의 다른 여러 컬럼을 사용해서 계산하기 위해 카테고리형 데이터를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebc4f9a7-7640-4172-8c35-1a6fb0c66bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "|   538800|     16458|   10|\n",
      "|   538942|     17346|   12|\n",
      "|  C539947|     13854|    1|\n",
      "|   540096|     13253|   16|\n",
      "|   540530|     14755|   27|\n",
      "|   541225|     14099|   19|\n",
      "|   541978|     13551|    4|\n",
      "|   542093|     17677|   16|\n",
      "|   543188|     12567|   63|\n",
      "|   543590|     17377|   19|\n",
      "|  C543757|     13115|    1|\n",
      "|  C544318|     12989|    1|\n",
      "|   544578|     12365|    1|\n",
      "|   545165|     16339|   20|\n",
      "|   545289|     14732|   30|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//송장번호(InvoiceNo)를 기준으로 그룹을 만들고 그룹별 물품 수를 카운트\n",
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea66d61-2bf6-430d-a528-cb259bfd1ff6",
   "metadata": {},
   "source": [
    "## 7.2.1 표현식을 이용한 그룹화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "463df6f8-7fbf-4985-9bea-bcff60ed45cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   536596|   6|              6|\n",
      "|   536938|  14|             14|\n",
      "|   537252|   1|              1|\n",
      "|   537691|  20|             20|\n",
      "|   538041|   1|              1|\n",
      "|   538184|  26|             26|\n",
      "|   538517|  53|             53|\n",
      "|   538879|  19|             19|\n",
      "|   539275|   6|              6|\n",
      "|   539630|  12|             12|\n",
      "|   540499|  24|             24|\n",
      "|   540540|  22|             22|\n",
      "|  C540850|   1|              1|\n",
      "|   540976|  48|             48|\n",
      "|   541432|   4|              4|\n",
      "|   541518| 101|            101|\n",
      "|   541783|  35|             35|\n",
      "|   542026|   9|              9|\n",
      "|   542375|   6|              6|\n",
      "|  C542604|   8|              8|\n",
      "+---------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.count\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "    count(\"Quantity\").alias(\"quan\"),\n",
    "    expr(\"count(Quantity)\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d480c66e-46b6-47fa-9561-878e238f14f4",
   "metadata": {},
   "source": [
    "## 7.2.2 맵을 이용한 그룹화\n",
    "- 컬럼을 키로, 수행할 집계 함수의 문자열을 값으로 하는 맵 타입을 사용해 트랜스포메이션을 정의할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f2ae6d8-de75-44eb-978a-d28fe99d0fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536596|               1.5|  1.1180339887498947|\n",
      "|   536938|33.142857142857146|  20.698023172885524|\n",
      "|   537252|              31.0|                 0.0|\n",
      "|   537691|              8.15|   5.597097462078001|\n",
      "|   538041|              30.0|                 0.0|\n",
      "|   538184|12.076923076923077|   8.142590198943392|\n",
      "|   538517|3.0377358490566038|  2.3946659604837897|\n",
      "|   538879|21.157894736842106|  11.811070444356483|\n",
      "|   539275|              26.0|  12.806248474865697|\n",
      "|   539630|20.333333333333332|  10.225241100118645|\n",
      "|   540499|              3.75|  2.6653642652865788|\n",
      "|   540540|2.1363636363636362|  1.0572457590557278|\n",
      "|  C540850|              -1.0|                 0.0|\n",
      "|   540976|10.520833333333334|   6.496760677872902|\n",
      "|   541432|             12.25|  10.825317547305483|\n",
      "|   541518| 23.10891089108911|  20.550782784878713|\n",
      "|   541783|11.314285714285715|   8.467657556242811|\n",
      "|   542026| 7.666666666666667|   4.853406592853679|\n",
      "|   542375|               8.0|  3.4641016151377544|\n",
      "|  C542604|              -8.0|  15.173990905493518|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(\"Quantity\"->\"avg\", \"Quantity\"->\"stddev_pop\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6371890-0405-4b37-ad64-90eaae7b95f6",
   "metadata": {},
   "source": [
    "# 7.3 윈도우 함수\n",
    "- 데이터의 특정 '윈도우'를 대상으로 고유의 집계 연산을 수행\n",
    "- 데이터의 '윈도우'는 현재 데이터에 대한 참조를 사용해 정의\n",
    "- 윈도우 명세는 함수에 전달될 로우를 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a69132-acd3-42f7-9c86-602c386eb747",
   "metadata": {},
   "source": [
    "- groupBy와의 차이점\n",
    "    - group-by 함수를 사용하면 모든 로우 레코드가 단일 그룹으로만 이동\n",
    "    - 윈도우 함수는 프레임에 입력되는 모든 로우에 대해 결괏값을 계산\n",
    "    - 프레임은 로우 그룹 기반의 테이블을 의미\n",
    "    - 각 로우는 하나 이상의 프레임에 할당될 수 있음\n",
    "- 스파크가 지원하는 세 가지 종류의 윈도우 함수\n",
    "    - 랭크 함수(ranking function)\n",
    "    - 분석 함수(analytic function)\n",
    "    - 집계 함수(aggregate function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb66f44f-9cbb-423e-96b6-08b0651157f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{col, to_date}\n",
       "dfWithDate: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{col, to_date}\n",
    "\n",
    "val dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041afc71-6c29-4c37-aa50-145f78e762f0",
   "metadata": {},
   "source": [
    "- 윈도우 함수를 정의하기 위해 첫 번째 단계로 윈도우 명세를 만듦\n",
    "- 여기서 사용하는 partitionBy 메서드는 지금까지 사용해온 파티셔닝 스키마의 개념과는 관련이 없으며 그룹을 어떻게 나눌지 결정하는 것과 유사한 개념\n",
    "- orderBy 메서드는 파티션의 정렬 방식을 정의\n",
    "- 그리고 프레임 명세(rowsBetween 구문)는 입력된 로우의 참조를 기반으로 프레임에 로우가 포함될 수 있는지 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f6fbe41-332b-4c01-91e8-cc1259f4042e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.functions.col\n",
       "windowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@14443fe0\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val windowSpec = Window\n",
    ".partitionBy(\"CustomerId\", \"date\")\n",
    ".orderBy(col(\"Quantity\").desc)\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a26b57-36da-48fb-98b1-fdfc53b519df",
   "metadata": {},
   "source": [
    "- 시간대별 최대 구매 개수를 구하는 예\n",
    "- 그러려면 위 예제에서 사용한 집계 함수에 컬럼명이나 표현식을 전달해야 함\n",
    "- 그리고 이 함수를 적용할 데이터 프레임이 정의된 윈도우 명세도 함께 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a9c5e22-1629-4afa-a7a1-25f03c5dd865",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.max\n",
       "maxPurchaseQuantity: org.apache.spark.sql.Column = max(Quantity) OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.max\n",
    "\n",
    "val maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6beefd47-398e-4ef3-97a6-7d7145c3090c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{dense_rank, rank}\n",
       "purchaseDenseRank: org.apache.spark.sql.Column = DENSE_RANK() OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n",
       "purchaseRank: org.apache.spark.sql.Column = RANK() OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{dense_rank, rank}\n",
    "\n",
    "val purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "val purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7762d6f7-313e-4be7-a203-61bd8c3e3c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|     12346|2011-01-18|   74215|           1|                1|              74215|\n",
      "|     12346|2011-01-18|  -74215|           2|                2|              74215|\n",
      "|     12347|2010-12-07|      36|           1|                1|                 36|\n",
      "|     12347|2010-12-07|      30|           2|                2|                 36|\n",
      "|     12347|2010-12-07|      24|           3|                3|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.col\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "\n",
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\n",
    ".select(\n",
    "    col(\"CustomerId\"),\n",
    "    col(\"date\"),\n",
    "    col(\"Quantity\"),\n",
    "    purchaseRank.alias(\"quantityRank\"),\n",
    "    purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "    maxPurchaseQuantity.alias(\"maxPurchaseQuantity\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057dc258-ebc9-4c8b-b421-7dd9c6d78fa6",
   "metadata": {},
   "source": [
    "# 7.4 그룹화 셋\n",
    "- 여러 그룹에 걸쳐 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6b4ac3c-47c1-4857-a788-f31d7bc7c1f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfNoNull: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfNoNull = dfWithDate.na.drop()\n",
    "\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41c5b6f1-da19-48b5-8343-1a5a08ce8806",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|CustomerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85040A|           48|\n",
      "|     18287|   85039B|          120|\n",
      "|     18287|   85039A|           96|\n",
      "|     18287|    84920|            4|\n",
      "|     18287|    84584|            6|\n",
      "|     18287|   84507C|            6|\n",
      "|     18287|   72351B|           24|\n",
      "|     18287|   72351A|           24|\n",
      "|     18287|   72349B|           60|\n",
      "|     18287|    47422|           24|\n",
      "|     18287|    47421|           48|\n",
      "|     18287|    35967|           36|\n",
      "|     18287|    23445|           20|\n",
      "|     18287|    23378|           24|\n",
      "|     18287|    23376|           48|\n",
      "|     18287|    23310|           36|\n",
      "|     18287|    23274|           12|\n",
      "|     18287|    23272|           12|\n",
      "|     18287|    23269|           36|\n",
      "+----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull\n",
    "GROUP BY CustomerId, stockcode\n",
    "ORDER BY CustomerId DESC, stockCode DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "842584d9-f03c-4dc7-b7fa-db11556c4c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|customerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85040A|           48|\n",
      "|     18287|   85039B|          120|\n",
      "|     18287|   85039A|           96|\n",
      "|     18287|    84920|            4|\n",
      "|     18287|    84584|            6|\n",
      "|     18287|   84507C|            6|\n",
      "|     18287|   72351B|           24|\n",
      "|     18287|   72351A|           24|\n",
      "|     18287|   72349B|           60|\n",
      "|     18287|    47422|           24|\n",
      "|     18287|    47421|           48|\n",
      "|     18287|    35967|           36|\n",
      "|     18287|    23445|           20|\n",
      "|     18287|    23378|           24|\n",
      "|     18287|    23376|           48|\n",
      "|     18287|    23310|           36|\n",
      "|     18287|    23274|           12|\n",
      "|     18287|    23272|           12|\n",
      "|     18287|    23269|           36|\n",
      "+----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//그룹화 셋을 사용해 동일한 작업 수행\n",
    "spark.sql(\"\"\"\n",
    "SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull\n",
    "GROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode))\n",
    "ORDER BY CustomerId DESC, stockCode DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d9d43b-c436-4fe1-9e4c-2040881e0597",
   "metadata": {},
   "source": [
    "- GROUPING SETS 구문은 SQL에서만 사용할 수 있음\n",
    "- DataFrame에서 동일한 연산을 수행하려면 rollup 메서드와 cube 메서드 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a7cee-6051-4d5b-8f0c-914ef2cab9ea",
   "metadata": {},
   "source": [
    "## 7.4.1 롤업\n",
    "- group-by 스타일의 다양한 연산을 수행할 수 있는 다차원의 집계 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23c4381a-b786-46ec-8c06-62760fd570a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|      Date|       Country|total_quantity|\n",
      "+----------+--------------+--------------+\n",
      "|      null|          null|       4906888|\n",
      "|2010-12-01|     Australia|           107|\n",
      "|2010-12-01|        Norway|          1852|\n",
      "|2010-12-01|        France|           449|\n",
      "|2010-12-01|       Germany|           117|\n",
      "|2010-12-01|          EIRE|           243|\n",
      "|2010-12-01|   Netherlands|            97|\n",
      "|2010-12-01|          null|         24032|\n",
      "|2010-12-01|United Kingdom|         21167|\n",
      "|2010-12-02|       Germany|           146|\n",
      "|2010-12-02|          EIRE|             4|\n",
      "|2010-12-02|          null|         20855|\n",
      "|2010-12-02|United Kingdom|         20705|\n",
      "|2010-12-03|      Portugal|            65|\n",
      "|2010-12-03|         Spain|           400|\n",
      "|2010-12-03|        Poland|           140|\n",
      "|2010-12-03|       Belgium|           528|\n",
      "|2010-12-03|   Switzerland|           110|\n",
      "|2010-12-03|        France|           239|\n",
      "|2010-12-03|         Italy|           164|\n",
      "+----------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rolledUpDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Date: date, Country: string ... 1 more field]\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//시간(신규 Date 컬럼)과 공간(Country 컬럼)을 축으로 하는 롤업 생성]\n",
    "//롤업의 결과로 생성된 DataFrame은 모든 날짜의 총합, 날짜별 총합, 날짜별 국가별 총합을 포함\n",
    "\n",
    "val rolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\"))\n",
    ".selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\")\n",
    ".orderBy(\"Date\")\n",
    "rolledUpDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c9d70ff-2f30-404f-956b-c2b700b8c2c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "//null 값을 가진 로우에서 전체 날짜의 합계 확인\n",
    "//롤업된 두 개의 컬럼값이 모두 null인 로우는 두 컬럼에 속한 레코드의 전체 합계를 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a38f2afc-0d40-44ea-810d-0114d9d74c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+\n",
      "|      Date|Country|total_quantity|\n",
      "+----------+-------+--------------+\n",
      "|      null|   null|       4906888|\n",
      "|2010-12-01|   null|         24032|\n",
      "|2010-12-02|   null|         20855|\n",
      "|2010-12-03|   null|         11548|\n",
      "|2010-12-05|   null|         16394|\n",
      "|2010-12-06|   null|         16095|\n",
      "|2010-12-07|   null|         19351|\n",
      "|2010-12-08|   null|         21275|\n",
      "|2010-12-09|   null|         16904|\n",
      "|2010-12-10|   null|         15388|\n",
      "|2010-12-12|   null|         10561|\n",
      "|2010-12-13|   null|         15234|\n",
      "|2010-12-14|   null|         17108|\n",
      "|2010-12-15|   null|         18169|\n",
      "|2010-12-16|   null|         29482|\n",
      "|2010-12-17|   null|         10517|\n",
      "|2010-12-19|   null|          3735|\n",
      "|2010-12-20|   null|         12617|\n",
      "|2010-12-21|   null|         10888|\n",
      "|2010-12-22|   null|          3053|\n",
      "+----------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF.where(\"Country IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60998423-a623-4122-ab56-eff3331dd8af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------+\n",
      "|Date|Country|total_quantity|\n",
      "+----+-------+--------------+\n",
      "|null|   null|       4906888|\n",
      "+----+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF.where(\"Date IS NULL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7e0eb9-2958-4338-8ebb-631990dc6fb7",
   "metadata": {},
   "source": [
    "## 7.4.2 큐브\n",
    "- 롤업을 고차원적으로 사용할 수 있게 해줌\n",
    "- 요소들을 계층적으로 다루는 대신 모든 차원에 대해 동일한 작업을 수행\n",
    "- 전체 기간에 대해 날짜와 국가별 결과를 얻을 수 있음\n",
    "    - 전체 날짜와 모든 국가에 대한 합계\n",
    "    - 모든 국가의 날짜별 합계\n",
    "    - 날짜별 국가별 합계\n",
    "    - 전체 날짜의 국가별 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b10ed4db-1806-4bb8-9e34-c1c0bd8b4a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------------+\n",
      "|Date|             Country|sum(Quantity)|\n",
      "+----+--------------------+-------------+\n",
      "|null|               Italy|         7999|\n",
      "|null|      United Kingdom|      4008533|\n",
      "|null|      Czech Republic|          592|\n",
      "|null|              Sweden|        35637|\n",
      "|null|             Finland|        10666|\n",
      "|null|             Lebanon|          386|\n",
      "|null|             Iceland|         2458|\n",
      "|null|             Denmark|         8188|\n",
      "|null|           Singapore|         5234|\n",
      "|null|                null|      4906888|\n",
      "|null|             Germany|       117448|\n",
      "|null|              Cyprus|         6317|\n",
      "|null|               Japan|        25218|\n",
      "|null|             Austria|         4827|\n",
      "|null|                EIRE|       136329|\n",
      "|null|        Saudi Arabia|           75|\n",
      "|null|           Lithuania|          652|\n",
      "|null|           Australia|        83653|\n",
      "|null|United Arab Emirates|          982|\n",
      "|null|              Norway|        19247|\n",
      "+----+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNoNull.cube(\"Date\", \"Country\").agg(sum(col(\"Quantity\")))\n",
    ".select(\"Date\", \"Country\", \"sum(Quantity)\").orderBy(\"Date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2f40bd-3fdd-4e16-95ee-02edcaa6799f",
   "metadata": {},
   "source": [
    "## 7.4.3 그룹화 메타데이터\n",
    "- 큐브와 롤업을 사용하다 보면 집계 수준에 따라 쉽게 필터링하기 위해 집계 수준을 조회하는 경우 발생\n",
    "- 이때 grouping_id를 사용\n",
    "- grouping_id는 결과 데이터셋의 집계 수준을 명시하는 컬럼을 제공\n",
    "    - 3 : 가장 높은 계층의 집계 결과에서 나타남. customerId나 stockCode에 관계 없이 총 수량을 제공\n",
    "    - 2 : 개별 재고 코드의 모든 집계 결과에서 나타남. customerId에 관계없이 재고 코드별 총 수량을 제공\n",
    "    - 1 : 구매한 물품에 관계없이 customerId를 기반으로 총 수량을 제공\n",
    "    - 0 : customerId와 stockCode별 조합에 따라 총 수량을 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab79b8d0-0a90-430d-b8cb-e55daad7e5fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+-------------+\n",
      "|customerId|stockCode|grouping_id()|sum(Quantity)|\n",
      "+----------+---------+-------------+-------------+\n",
      "|      null|     null|            3|      4906888|\n",
      "|      null|    21002|            2|           13|\n",
      "|      null|   84659A|            2|          133|\n",
      "|      null|    21034|            2|         1872|\n",
      "|      null|    21619|            2|          900|\n",
      "|      null|   85049C|            2|         1198|\n",
      "|      null|    82582|            2|         4271|\n",
      "|      null|    22643|            2|          185|\n",
      "|      null|    22642|            2|          253|\n",
      "|      null|    22717|            2|         1170|\n",
      "|      null|    21269|            2|           28|\n",
      "|      null|    21383|            2|          770|\n",
      "|      null|    22967|            2|         1139|\n",
      "|      null|    22080|            2|         1172|\n",
      "|      null|    20713|            2|        10223|\n",
      "|      null|    85141|            2|          114|\n",
      "|      null|    22365|            2|          728|\n",
      "|      null|    37449|            2|          512|\n",
      "|      null|   85180B|            2|          196|\n",
      "|      null|    22065|            2|         6430|\n",
      "+----------+---------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{grouping_id, sum, expr}\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{grouping_id, sum, expr}\n",
    "\n",
    "dfNoNull.cube(\"customerId\", \"stockCode\").agg(grouping_id(), sum(\"Quantity\"))\n",
    ".orderBy(col(\"grouping_id()\").desc)\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63c4032-7c97-4bd2-ad4c-793be5f3c6f0",
   "metadata": {},
   "source": [
    "## 7.4.4 피벗\n",
    "- 로우를 컬럼으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb335fb8-69fc-4199-b1ef-6500fd23a737",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pivoted: org.apache.spark.sql.DataFrame = [date: date, Australia_sum(Quantity): bigint ... 113 more fields]\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7db14c23-d3f4-48b9-82f5-14959e4feb2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/23 15:47:02 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+----------+-----------------+\n",
      "|      date|USA_sum(Quantity)|\n",
      "+----------+-----------------+\n",
      "|2011-12-06|             null|\n",
      "|2011-12-09|             null|\n",
      "|2011-12-08|             -196|\n",
      "|2011-12-07|             null|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted.where(\"date > '2011-12-05'\").select(\"date\", \"`USA_sum(Quantity)`\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e0117-ed33-43f8-b66c-8062b095d6a1",
   "metadata": {},
   "source": [
    "# 7.5 사용자 정의 집계 함수\n",
    "- 스파크는 입력 데이터의 모든 그룹의 중간 결과를 단일 AggregationBuffer에 저장해 관리\n",
    "- UDAF를 생성하려면 기본 클래스인 UserDefinedAggregateFunction을 상속받음\n",
    "- 그리고 다음과 같은 메서드 정의\n",
    "    - inputSchema\n",
    "    - bufferSchema\n",
    "    - dataType\n",
    "    - deterministic\n",
    "    - initialize\n",
    "    - update\n",
    "    - merge\n",
    "    - evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bda664d1-416e-4ae0-bdcd-3b831aec3bde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.MutableAggregationBuffer\n",
       "import org.apache.spark.sql.expressions.UserDefinedAggregateFunction\n",
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.types._\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.MutableAggregationBuffer\n",
    "import org.apache.spark.sql.expressions.UserDefinedAggregateFunction\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d131b98-eea0-47a3-9c15-463ec0903719",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class BoolAnd\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BoolAnd extends UserDefinedAggregateFunction{\n",
    "    def inputSchema: org.apache.spark.sql.types.StructType =\n",
    "    StructType(StructField(\"value\", BooleanType) :: Nil)\n",
    "    def bufferSchema: StructType = StructType(\n",
    "        StructField(\"result\", BooleanType) :: Nil)\n",
    "    def dataType: DataType = BooleanType\n",
    "    def deterministic: Boolean = true\n",
    "    def initialize(buffer: MutableAggregationBuffer): Unit = {\n",
    "        buffer(0) = true\n",
    "    }\n",
    "    def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n",
    "        buffer(0) = buffer.getAs[Boolean](0) && input.getAs[Boolean](0)\n",
    "    }\n",
    "    def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n",
    "        buffer1(0) = buffer1.getAs[Boolean](0) && buffer2.getAs[Boolean](0)\n",
    "    }\n",
    "    def evaluate(buffer: Row): Any = {\n",
    "        buffer(0)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c3c05550-0b21-4f32-95d7-c3e93ec16891",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ba: BoolAnd = BoolAnd@44e91381\n",
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ba = new BoolAnd\n",
    "spark.udf.register(\"booland\", ba)\n",
    "\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ef9f7a6-4a64-4ef3-ae8a-77518417ac96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|booland(t)|booland(f)|\n",
      "+----------+----------+\n",
      "|      true|     false|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1)\n",
    ".selectExpr(\"explode(array(TRUE, TRUE, TRUE)) as t\")\n",
    ".selectExpr(\"explode(array(TRUE, FALSE, TRUE)) as f\", \"t\")\n",
    ".select(ba(col(\"t\")), expr(\"booland(f)\"))\n",
    ".show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

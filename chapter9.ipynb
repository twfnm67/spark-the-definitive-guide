{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea46e481-a4bc-4af2-b91a-cdddab6b2b21",
   "metadata": {},
   "source": [
    "- 스파크에는 여섯 가지 '핵심' 데이터소스와 커뮤니티에서 만든 수백 가지 외부 데이터 소스가 있음\n",
    "    - 다양한 데이터소스를 읽고 쓰는 능력과 데이터 소스를 커뮤니티에서 자체적으로 만들어내는 능력은 스파크를 가장 강력하게 만들어주는 힘\n",
    "- 스파크의 핵심 데이터소스\n",
    "    - CSV\n",
    "    - JSON\n",
    "    - 파케이\n",
    "    - ORC\n",
    "    - JDBC/ODBC 연결\n",
    "    - 일반 텍스트파일\n",
    "- 커뮤니티에서 만든 수많은 데이터 소스 중 일부\n",
    "    - 카산드라\n",
    "    - HBase\n",
    "    - 몽고디비\n",
    "    - AWS Redshift\n",
    "    - XML\n",
    "    - 기타 수많은 데이터소스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4634783e-04c8-416b-922f-6cb2f2b65b2d",
   "metadata": {},
   "source": [
    "# 9.1 데이터소스 API의 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e777ce5e-a848-4b19-93b0-90179dd044fe",
   "metadata": {},
   "source": [
    "## 9.1.1 읽기 API 구조\n",
    "- 데이터 읽기의 핵심 구조\n",
    "    DataFrameReader.format(...).option(\"key\", \"value\").schema(...).load()\n",
    "- 모든 데이터소스를 읽을 때 위와 같은 형식 사용\n",
    "    - format 메서드는 선택적으로 사용할 수 있으며, 기본값은 파케이 포맷\n",
    "    - option 메서드를 사용해 데이터를 읽는 방법에 대한 파라미터를 키-값 쌍으로 설정\n",
    "    - schema 메서드는 데이터 소스에서 스키마를 제공하거나, 스키마 추론 기능을 사용하려는 경우에 선택적으로 사용할 수 있음\n",
    "    - 데이터 포맷별로 필요한 몇 가지 옵션이 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f97860d-5648-4674-bd30-beea9ddd3ebe",
   "metadata": {},
   "source": [
    "## 9.1.2 데이터 읽기의 기초\n",
    "- 스파크에서 데이터를 읽을 때는 기본적으로 DataFrameReader를 사용\n",
    "- DataFrameReader는 SparkSession의 read 속성으로 접근\n",
    "    Spark.read\n",
    "- DataFrameReader를 얻은 다음에는 다음과 같은 값을 지정\n",
    "    - 포맷\n",
    "    - 스키마\n",
    "    - 읽기 모드\n",
    "    - 옵션\n",
    "- 포맷, 스키마, 그리고 옵션은 트랜스포메이션을 추가로 정의할 수 있는 DataFrameReader를 반환\n",
    "- 그리고 읽기 모드를 제외한 세가지 항목은 필요한 경우에만 선택적으로 지정할 수 있음\n",
    "- 데이터소스마다 데이터를 읽는 방식을 결정할 수 있는 옵션 제공\n",
    "- 사용자는 DataFrameReader에 반드시 데이터를 읽을 경로를 지정해야 함\n",
    "### 읽기 모드\n",
    "- 스파크가 형식에 맞지 않는 데이터를 만났을 때의 동작 방식을 지정하는 옵션\n",
    "    - Permissive : 오류 레코드의 모든 필드를 null로 설정하고 모든 오류 레코드를 \\_corrupt_record라는 문자열 컬럼에 기록\n",
    "    - dropMalformed : 형식에 맞지 않는 레코드가 포함된 로우 제거\n",
    "    - failFast : 형식에 맞지 않는 레코드를 만나면 즉시 종료\n",
    "- 읽기 모드의 기본값은 Permissive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926a5c6-3179-4683-baa1-9d135bd96341",
   "metadata": {},
   "source": [
    "## 9.1.3 쓰기 API 구조\n",
    "- 데이터 쓰기의 핵심 구조\n",
    "    DataFrameWriter.format(...).option(...).partitonBy(...).bucketBy(...).sortBy(...).save()\n",
    "- format 메서드는 선택적으로 사용할 수 있으며 기본값은 파케이 포맷\n",
    "- option 메서드를 사용해 데이터 쓰기 방법을 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e12cf6-7d16-49bc-945b-47c5ca186c68",
   "metadata": {},
   "source": [
    "## 9.1.4 데이터 쓰기의 기초\n",
    "- 데이터 쓰기는 데이터 읽기와 매우 유사\n",
    "    - DataFrameReader 대신 DataFrameWriter를 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b1b18-2da2-4f56-a438-1966af27c749",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 저장 모드\n",
    "- 저장 모드는 스파크가 지정된 위치에서 동일한 파일이 발견했을 때의 동작 방식을 지정하는 옵션\n",
    "    - append : 해당 경로에 이미 존재하는 파일 목록에 결과 파일을 추가\n",
    "    - overwrite : 이미 존재하는 모든 데이터를 완전히 덮어 씀\n",
    "    - errorIfExists : 해당 경로에 데이터나 파일이 존재하는 경우 오류를 발생시키면서 쓰기 작업 실패\n",
    "    - ignore : 해당 경로에 데이터나 파일이 존재하는 경우 아무런 처리도 하지 않음\n",
    "- 기본값은 errorIfExists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c441fe9-c342-4390-baca-56e620919c86",
   "metadata": {},
   "source": [
    "# 9.2 CSV 파일\n",
    "- comma-separated values. 콤마(,)로 구분된 값\n",
    "- 각 줄이 단일 레코드가 되며 레코드의 각 필드를 콤마로 구분하는 일반적인 텍스트 파일 포맷"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fdd5c8-6ce8-491a-9edf-b581e7388f9b",
   "metadata": {},
   "source": [
    "## 9.2.1 CSV 옵션\n",
    "- 교재 pg 250-251"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5187be-9a06-4856-9a1b-f4d4da6af90c",
   "metadata": {},
   "source": [
    "## 9.2.2 CSV 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9203517-9632-411b-a715-d9bda04e6e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@44409a29\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfd8a02c-c644-4bf7-9294-eb0320a0780d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53b22e0d-92b6-40cb-8fcc-61fc8dc88202",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myManualSchem: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,false))\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myManualSchem = new StructType(Array(\n",
    "    new StructField(\"DEST_COUNTRY_NAME\", StringType, true),\n",
    "    new StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true),\n",
    "    new StructField(\"count\", LongType, false)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6b2c58d-71b9-4923-aba7-cd5bdcff8821",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"csv\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"mode\", \"FAILFAST\")\n",
    ".schema(myManualSchem)\n",
    ".load(\"../../../Downloads/Spark-The-Definitive-Guide-master/data/flight-data/csv/2010-summary.csv\")\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a0e67e5-d9d5-49cd-9817-1022deb432e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myManualSchema: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,LongType,true),StructField(ORIGIN_COUNTRY_NAME,LongType,true),StructField(count,LongType,false))\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myManualSchema = new StructType(Array(\n",
    "    new StructField(\"DEST_COUNTRY_NAME\", LongType, true),\n",
    "    new StructField(\"ORIGIN_COUNTRY_NAME\", LongType, true),\n",
    "    new StructField(\"count\", LongType, false)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5ed3337-16e9-411f-b253-178fa7af98bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/21 21:36:01 ERROR Executor: Exception in task 0.0 in stage 12.0 (TID 19)\n",
      "org.apache.spark.sql.execution.QueryExecutionException: Encountered error while reading file file:///Users/choyubin/Downloads/Spark-The-Definitive-Guide-master/data/flight-data/csv/2010-summary.csv. Details: \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:731)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "Caused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n",
      "\t... 16 more\n",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"United States\"\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n",
      "\t... 22 more\n",
      "Caused by: java.lang.NumberFormatException: For input string: \"United States\"\n",
      "\tat java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)\n",
      "\tat java.base/java.lang.Long.parseLong(Long.java:708)\n",
      "\tat java.base/java.lang.Long.parseLong(Long.java:831)\n",
      "\tat scala.collection.immutable.StringLike.toLong(StringLike.scala:309)\n",
      "\tat scala.collection.immutable.StringLike.toLong$(StringLike.scala:309)\n",
      "\tat scala.collection.immutable.StringOps.toLong(StringOps.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8(UnivocityParser.scala:166)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8$adapted(UnivocityParser.scala:166)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:259)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$7(UnivocityParser.scala:166)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:312)\n",
      "\t... 25 more\n",
      "23/03/21 21:36:01 WARN TaskSetManager: Lost task 0.0 in stage 12.0 (TID 19) (192.168.0.2 executor driver): org.apache.spark.sql.execution.QueryExecutionException: Encountered error while reading file file:///Users/choyubin/Downloads/Spark-The-Definitive-Guide-master/data/flight-data/csv/2010-summary.csv. Details: \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:731)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "Caused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n",
      "\t... 16 more\n",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"United States\"\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n",
      "\t... 22 more\n",
      "Caused by: java.lang.NumberFormatException: For input string: \"United States\"\n",
      "\tat java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)\n",
      "\tat java.base/java.lang.Long.parseLong(Long.java:708)\n",
      "\tat java.base/java.lang.Long.parseLong(Long.java:831)\n",
      "\tat scala.collection.immutable.StringLike.toLong(StringLike.scala:309)\n",
      "\tat scala.collection.immutable.StringLike.toLong$(StringLike.scala:309)\n",
      "\tat scala.collection.immutable.StringOps.toLong(StringOps.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8(UnivocityParser.scala:166)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8$adapted(UnivocityParser.scala:166)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:259)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$7(UnivocityParser.scala:166)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:312)\n",
      "\t... 25 more\n",
      "\n",
      "23/03/21 21:36:01 ERROR TaskSetManager: Task 0 in stage 12.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 19) (192.168.0.2 executor driver): org.apache.spark.sql.execution.QueryExecutionException: Encountered error while reading file file:///Users/choyubin/Downloads/Spark-The-Definitive-Guide-master/data/flight-data/csv/2010-summary.csv. Details:",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 19) (192.168.0.2 executor driver): org.apache.spark.sql.execution.QueryExecutionException: Encountered error while reading file file:///Users/choyubin/Downloads/Spark-The-Definitive-Guide-master/data/flight-data/csv/2010-summary.csv. Details:",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:731)",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)",
      "Caused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)",
      "\t... 16 more",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"United States\"",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:330)",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)",
      "\t... 22 more",
      "Caused by: java.lang.NumberFormatException: For input string: \"United States\"",
      "\tat java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)",
      "\tat java.base/java.lang.Long.parseLong(Long.java:708)",
      "\tat java.base/java.lang.Long.parseLong(Long.java:831)",
      "\tat scala.collection.immutable.StringLike.toLong(StringLike.scala:309)",
      "\tat scala.collection.immutable.StringLike.toLong$(StringLike.scala:309)",
      "\tat scala.collection.immutable.StringOps.toLong(StringOps.scala:33)",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8(UnivocityParser.scala:166)",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8$adapted(UnivocityParser.scala:166)",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:259)",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$7(UnivocityParser.scala:166)",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:312)",
      "\t... 25 more",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2863)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:3084)",
      "  ... 34 elided",
      "Caused by: org.apache.spark.sql.execution.QueryExecutionException: Encountered error while reading file file:///Users/choyubin/Downloads/Spark-The-Definitive-Guide-master/data/flight-data/csv/2010-summary.csv. Details:",
      "  at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:731)",
      "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)",
      "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)",
      "  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:136)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)",
      "  ... 1 more",
      "Caused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.",
      "  at org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)",
      "  at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)",
      "  at org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)",
      "  at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)",
      "  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)",
      "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)",
      "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)",
      "  ... 16 more",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"United States\"",
      "  at org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:330)",
      "  at org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)",
      "  at org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)",
      "  at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)",
      "  ... 22 more",
      "Caused by: java.lang.NumberFormatException: For input string: \"United States\"",
      "  at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)",
      "  at java.base/java.lang.Long.parseLong(Long.java:708)",
      "  at java.base/java.lang.Long.parseLong(Long.java:831)",
      "  at scala.collection.immutable.StringLike.toLong(StringLike.scala:309)",
      "  at scala.collection.immutable.StringLike.toLong$(StringLike.scala:309)",
      "  at scala.collection.immutable.StringOps.toLong(StringOps.scala:33)",
      "  at org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8(UnivocityParser.scala:166)",
      "  at org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8$adapted(UnivocityParser.scala:166)",
      "  at org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:259)",
      "  at org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$7(UnivocityParser.scala:166)",
      "  at org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:312)",
      "  ... 25 more",
      ""
     ]
    }
   ],
   "source": [
    "spark.read.format(\"csv\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"mode\", \"FAILFAST\")\n",
    ".schema(myManualSchema)\n",
    ".load(\"../../../Downloads/Spark-The-Definitive-Guide-master/data/flight-data/csv/2010-summary.csv\")\n",
    ".take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b72b9-10f7-45f0-81bd-d400a7499dd2",
   "metadata": {},
   "source": [
    "## 9.2.3 CSV 파일 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9925a2e3-15c5-48ae-adea-4fea957f6cad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csvFile: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvFile = spark.read.format(\"csv\")\n",
    ".option(\"header\", \"true\").option(\"mode\", \"FAILFAST\").schema(myManualSchem)\n",
    ".load(\"../../../Downloads/Spark-The-Definitive-Guide-master/data/flight-data/csv/2010-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fccddcb-cfaf-423f-b919-fc430dc58978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\",\"\\t\")\n",
    ".save(\"/tmp/my-tsv-file.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7f2684-4d34-4f62-b71c-0d471493f7d8",
   "metadata": {},
   "source": [
    "# 9.3 JSON 파일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c549acd-0ee6-4f37-a82e-219b6ddc188c",
   "metadata": {},
   "source": [
    "## 9.3.1 JSON 옵션\n",
    "- 교재 pg 255-256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c51c6-67eb-4c2b-aac1-8fd273d746c7",
   "metadata": {},
   "source": [
    "## 9.3.2 JSON 파일 읽기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc15b03e-4de9-4ff4-a87a-0bfad770bd5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"json\").option(\"mode\", \"FAILFAST\").schema(myManualSchem)\n",
    ".load(\"../../../Downloads/Spark-The-Definitive-Guide-master/data/flight-data/json/2010-summary.json\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c579c83-e0e9-42a6-9833-35f2790c99f3",
   "metadata": {},
   "source": [
    "## 9.3.3 JSON 파일 쓰기\n",
    "- 파티션당 하나의 파일을 만들며 전체 DataFrame을 단일 폴더에 저장\n",
    "- JSON 객체는 한 줄에 하나씩 기록됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56c3a707-e0d2-458f-be66-eb41935d78bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csvFile.write.format(\"json\").mode(\"overwrite\").save(\"/tmp/my-json-file.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad41897c-9ec1-4f79-b20d-9dfafed5e32b",
   "metadata": {},
   "source": [
    "# 9.4 파케이 파일\n",
    "- 컬럼 기반의 데이터 저장 방식\n",
    "- 분석 워크로드에 최적화\n",
    "- 저장소 공간 절약, 전체 파일을 읽는 대신 개별 컬럼을 읽을 수 있으며, 컬럼 기반의 압축 기능 제공\n",
    "- 아파치 스파크와 잘 호환되기 때뭉네 스파크의 기본 파일 포맷\n",
    "- 읽기 연산 시 JSON이나 CSV보다 훨씬 효율적으로 동작하므로 장기 저장용 데이터는 파케이 포맷으로 저장하는 것이 좋음\n",
    "- 복합 데이터 타입을 지원\n",
    "    - 컬럼이 배열, 맵, 구조체 데이터 타입이라 해도 문제없이 읽고 쓸 수 있음\n",
    "    - csv에서는 배열 사용 불가\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd19fbc1-0b5f-47fe-b9da-9eec16504d7b",
   "metadata": {},
   "source": [
    "## 9.4.1 파케이 파일 읽기\n",
    "- 파케이는 옵션이 거의 없음\n",
    "    - 데이터를 저장할 때 자체 스키마를 사용해 데이터를 저장하기 때문\n",
    "    - 포맷을 설정하는 것만으로 충분\n",
    "- DataFrame을 표현하기 위해 정확한 스키마가 필요한 경우에만 스키마를 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea919605-cc2a-47a0-9b5b-da46dfc1c171",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@2ee04fbc\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ae848f7-f751-48e3-a3b4-c10f800d9ede",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"parquet\").load(\"../../../Downloads/Spark-The-Definitive-Guide-master/data/flight-data/parquet/2010-summary.parquet\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba07831-34ce-4da7-afa8-0be47ffdae63",
   "metadata": {},
   "source": [
    "### 파케이 옵션\n",
    "- 교재 pg 259"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e5337-de0d-4342-9e32-3047f644b0bd",
   "metadata": {},
   "source": [
    "## 9.4.2 파케이 파일 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6909a98-fb29-4695-87e4-06a622bda787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csvFile.write.format(\"parquet\").mode(\"overwrite\").save(\"/tmp/my-parquet-file.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dda375-2486-4832-b66a-12db4b93896d",
   "metadata": {},
   "source": [
    "# 9.5 ORC 파일\n",
    "- ORC는 하둡 워크로드를 위해 셜계된 자기 기술적이며 데이터 타입을 인식할 수 있는 컬럼 기반의 파일 포맷\n",
    "- 이 포맷은 대규모 스트리밍 읽기에 최적화되어 있을 뿐만 아니라 필요한 로우를 신속하게 찾아낼 수 있는 기능이 통합되어 있음\n",
    "- 스파크는 ORC 파일 포맷을 효율적으로 사용할 수 있으므로 별도의 옵션 지정 없이 데이터를 읽을 수 있음\n",
    "- 파케이는 스파크에 최적화된 반면 ORC는 하이브에 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34136b48-0096-4f89-b076-92d607be5924",
   "metadata": {},
   "source": [
    "## 9.5.1 ORC 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3060291b-7169-49be-ba7c-a01cdd5e58ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"orc\").load(\"../../../Downloads/Spark-The-Definitive-Guide-master/data/flight-data/orc/2010-summary.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf6fbc7-852c-4307-aca4-cada743a2292",
   "metadata": {},
   "source": [
    "## 9.5.2 ORC 파일 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73b9783e-a98a-4a49-b968-16e0c6c125a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csvFile.write.format(\"orc\").mode(\"overwrite\").save(\"/tmp/my-orc-file.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae047c-a2c2-4879-935b-ee919f297343",
   "metadata": {},
   "source": [
    "# 9.6 SQL 데이터베이스\n",
    "- SQL 데이터소스는 매우 강력한 커넥터 중 하나\n",
    "    - 사용자는 SQL을 지원하는 다양한 시스템에 SQL 데이터소스를 연결\n",
    "    - e.g. MySQL, PostgreSQL, Oracle 데이터베이스에 접속, SQLite에 접속\n",
    "- 데이터베이스는 원시 파일 형태가 아니므로 고려해야 할 옵션이 더 많음\n",
    "    - e.g. 데이터베이스의 인증 정보나 접속과 관련된 옵션 필요\n",
    "    - 스파크 클러스터에서 데이터베이스 시스템에 접속 가능한지 네트워크 상태 확인 필요\n",
    "- 데이터베이스를 설정하는 번거로움을 없애고 이 책의 목적에 충실하기 위해 SQLite 실행을 위한 참고용 샘플 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cdc006-6245-4464-87c7-2a2b913597ed",
   "metadata": {},
   "source": [
    "### JDBC 데이터소스 옵션\n",
    "- 교재 pg 262-263 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f668e2-b61d-42b5-b348-6270f3c35107",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9.6.1 SQL 데이터베이스 읽기\n",
    "- 파일 읽기와 마찬가지로 SQL 데이터베이스에서 데이터를 읽는 방법은 다른 데이터소스와 다르지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acf8ddd8-83da-400f-bb9b-5c9298ddfc4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "driver: String = org.sqlite.JDBC\n",
       "path: String = /Users/choyubin/Downloads/Spark-The-Definitive-Guide-master/data/flight-data/jdbc/my-sqlite.db\n",
       "url: String = jdbc:sqlite://Users/choyubin/Downloads/Spark-The-Definitive-Guide-master/data/flight-data/jdbc/my-sqlite.db\n",
       "tablename: String = flight_info\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val driver = \"org.sqlite.JDBC\"\n",
    "val path = \"/Users/choyubin/Downloads/Spark-The-Definitive-Guide-master/data/flight-data/jdbc/my-sqlite.db\"\n",
    "val url = s\"jdbc:sqlite:/${path}\"\n",
    "val tablename = \"flight_info\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f68027-009c-46b6-85da-3b22bdd751d4",
   "metadata": {},
   "source": [
    "- 접속 관련 속성을 정의한 다음, 정상적으로 데이터베이스에 접속되는지 테스트해 해당 연결이 유효한지 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b2a5e8-4ff7-487e-b093-380bd0c92ba5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dbDataFrame: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dbDataFrame = spark.read.format(\"jdbc\").option(\"url\", url)\n",
    ".option(\"dbtable\", tablename).option(\"driver\", driver).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f39e7b95-deac-42ef-9c9e-b077e2220aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|         Anguilla|\n",
      "|           Russia|\n",
      "|         Paraguay|\n",
      "|          Senegal|\n",
      "|           Sweden|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbDataFrame.select(\"DEST_COUNTRY_NAME\").distinct().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57957066-018a-476a-a96b-7e38b36920af",
   "metadata": {},
   "source": [
    "## 9.6.2 쿼리 푸시다운\n",
    "- 스파크는 DataFrame을 만들기 전에 데이터베이스 자체에서 데이터를 필터링하도록 만들 수 있음\n",
    "- 스파크는 특정 유형의 쿼리를 더 나은 방식으로 처리할 수 있음\n",
    "    - 예를 들어, DataFrame에 필터를 명시하면 스파크는 해당 필터에 대한 처리를 데이터베이스로 위임(push down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a97588de-8748-4a93-b880-1900bde166cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation(flight_info) [numPartitions=1] [DEST_COUNTRY_NAME#0,ORIGIN_COUNTRY_NAME#1,count#2] PushedFilters: [*In(DEST_COUNTRY_NAME, [Anuguilla,Sweden])], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:decimal(20,0)>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbDataFrame.filter(\"DEST_COUNTRY_NAME in ('Anuguilla', 'Sweden')\").explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e92fe76b-7920-4fe2-a5da-775746461056",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pushdownQuery: String = (SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info) AS flight_info\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pushdownQuery = \"\"\"(SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info) AS flight_info\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da6651cc-d145-4b1f-836b-3d5508c177b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dbDataFrame: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dbDataFrame = spark.read.format(\"jdbc\")\n",
    ".option(\"url\", url).option(\"dbtable\", pushdownQuery).option(\"driver\", driver)\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa544e33-c705-44d1-88c3-bb61cbed29c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation((SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info) AS flight_info) [numPartitions=1] [DEST_COUNTRY_NAME#12] PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbDataFrame.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea41111-be16-446e-a042-f35e08cc31e0",
   "metadata": {},
   "source": [
    "### 데이터베이스 병렬로 읽기\n",
    "- 스파크는 파일 크기, 파일 유형 그리고 압축 방식에 따른 '분할 가능성'에 따라 여러 파일을 읽어 하나의 파티션으로 만들거나 여러 파티션을 하나의 파일로 만드는 기본 알고리즘을 가지고 있음\n",
    "    - 파일이 가진 이런 유연성은 SQL 데이터베이스에도 존재하지만 몇 가지 수동 설정이 필요\n",
    "    - numPartitions 옵션을 사용해 읽기 및 쓰기용 동시 작업 수를 제한할 수 있는 최대 파티션 수를 설정할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70322062-2ac8-43ad-a00a-8e2d0d659273",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dbDataFrame: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dbDataFrame = spark.read.format(\"jdbc\")\n",
    ".option(\"url\", url).option(\"dbtable\", tablename).option(\"driver\", driver)\n",
    ".option(\"numPartitions\", 10).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81712355-706e-48f5-abe5-ac5884f6e340",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "props: java.util.Properties = {driver=org.sqlite.JDBC}\n",
       "res4: Object = null\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val props = new java.util.Properties\n",
    "props.setProperty(\"driver\", \"org.sqlite.JDBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "691f21a3-3502-485b-bdac-89e718f8c3c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicates: Array[String] = Array(DEST_COUNTRY_NAME = 'Sweden' OR ORIGIN_COUNTRY_NAME = 'Sweden', DEST_COUNTRY_NAME = 'Anguilla' or ORIGIN_COUNTRY_NAME = 'Anguilla')\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predicates = Array(\n",
    "    \"DEST_COUNTRY_NAME = 'Sweden' OR ORIGIN_COUNTRY_NAME = 'Sweden'\",\n",
    "    \"DEST_COUNTRY_NAME = 'Anguilla' or ORIGIN_COUNTRY_NAME = 'Anguilla'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b559ce8-924e-4612-8641-b6374dc9a503",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|           Sweden|      United States|   65|\n",
      "|    United States|             Sweden|   73|\n",
      "|         Anguilla|      United States|   21|\n",
      "|    United States|           Anguilla|   20|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.jdbc(url, tablename, predicates, props).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d94a2c9-27b1-4348-80dc-af1ffe03cea5",
   "metadata": {},
   "source": [
    "- 연관성 없는 조건절을 정의하면 중복 로우가 많이 발생할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6e80f94-e514-4f28-b1b0-e41f834dee68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "props: java.util.Properties = {driver=org.sqlite.JDBC}\n",
       "res6: Object = null\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val props = new java.util.Properties\n",
    "props.setProperty(\"driver\", \"org.sqlite.JDBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57f0ef34-366b-4aab-a529-23d8d743040c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicates: Array[String] = Array(DEST_COUNTRY_NAME != 'Sweden' OR ORIGIN_COUNTRY_NAME != 'Sweden', DEST_COUNTRY_NAME != 'Anguilla' OR ORIGIN_COUNTRY_NAME != 'Anguilla')\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predicates = Array(\n",
    "    \"DEST_COUNTRY_NAME != 'Sweden' OR ORIGIN_COUNTRY_NAME != 'Sweden'\",\n",
    "    \"DEST_COUNTRY_NAME != 'Anguilla' OR ORIGIN_COUNTRY_NAME != 'Anguilla'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b522082-ade8-4339-b991-88775fc080c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Long = 510\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.jdbc(url, tablename, predicates, props).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf322a9c-b8b7-4385-933d-26988abf7e28",
   "metadata": {},
   "source": [
    "### 슬라이딩 윈도우 기반의 파티셔닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f31af49-10d4-42b0-b838-51a58fd229cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colName: String = count\n",
       "lowerBound: Long = 0\n",
       "upperBound: Long = 348113\n",
       "numPartitions: Int = 10\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colName = \"count\"\n",
    "val lowerBound = 0L\n",
    "val upperBound = 348113L\n",
    "val numPartitions = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afe3e511-e484-4259-95a2-2a737cb03dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Long = 255\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.jdbc(url, tablename, colName, lowerBound, upperBound, numPartitions, props)\n",
    ".count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7341b8-22a8-43a7-9c3a-7ef6a16d76e6",
   "metadata": {},
   "source": [
    "## 9.6.3 SQL 데이터베이스 쓰기\n",
    "- URI를 지정하고 지정한 쓰기 모드에 따라 데이터를 쓰면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e408fa24-f388-49af-ba91-231f85059030",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/21 21:36:23 WARN JdbcUtils: Requested isolation level 1 is not supported; falling back to default isolation level 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newPath: String = jdbc:sqlite://tmp/my-sqlite.db\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newPath = \"jdbc:sqlite://tmp/my-sqlite.db\"\n",
    "csvFile.write.mode(\"overwrite\").jdbc(newPath, tablename, props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2831eb68-c407-40b0-9ee5-9ead07dedc8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: Long = 255\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.jdbc(newPath, tablename, props).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3b92c2c-51fd-4f80-85b5-67267af88102",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/21 21:37:03 WARN JdbcUtils: Requested isolation level 1 is not supported; falling back to default isolation level 8\n"
     ]
    }
   ],
   "source": [
    "csvFile.write.mode(\"append\").jdbc(newPath, tablename, props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f28b24-a02a-4f85-b5ef-06af7b5c056d",
   "metadata": {},
   "source": [
    "# 9.7 텍스트 파일\n",
    "- 스파크는 일반 텍스트 파일도 읽을 수 있음\n",
    "    - 파일의 각 줄은 DataFrame의 각 레코드가 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3504ac4-85cd-4dd7-9cef-75cc544b583f",
   "metadata": {},
   "source": [
    "## 9.7.1 텍스트 파일 읽기\n",
    "- textfile 메서드에 텍스트 파일을 지정하기만 하면 됨\n",
    "    - textfile 메서드는 파티션 수행 결과로 만들어진 디렉터리명을 무시\n",
    "    - 파티션된 텍스트 파일을 읽거나 쓰려면 읽기 및 쓰기 시 파티션 수행 결과로 만들어진 디렉터리를 인식할 수 있도록 text 메서드를 사용해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d4b546-a844-4fc0-8b28-10f7cbecc893",
   "metadata": {
    "tags": []
   },
   "source": [
    "spark.read.textFile(\"/Users/choyubin/Downloads/Spark-The-Definitive-Guide-master/data/flight-data/csv/2010-summary.csv\")\n",
    ".selectExpr(\"split(value ',') as rows\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c107232c-629b-47c6-bd69-b28080e1db22",
   "metadata": {},
   "source": [
    "## 9.7.2 텍스트 파일 쓰기\n",
    "- 텍스트 파일을 쓸 때는 문자열 컬럼이 하나만 존재해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba795003-d709-4590-a642-73ba8034e91a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csvFile.select(\"DEST_COUNTRY_NAME\").write.text(\"/tmp/simple-text-file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e55ab467-94e8-4ca2-94ae-02119ed0b128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csvFile.limit(10).select(\"DEST_COUNTRY_NAME\",\"count\")\n",
    ".write.partitionBy(\"count\").text(\"/tmp/five-csv-files2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a833266-9479-4386-ae29-43f432aaebd2",
   "metadata": {},
   "source": [
    "# 9.8 고급 I/O 개념\n",
    "- 쓰기 작업 전에 파티션 수를 조절함으로써 병렬로 처리할 파일 수를 제어할 수 있음\n",
    "    - 버켓팅\n",
    "    - 파티셔닝\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff3ed18-3111-4a02-81f3-4cdf95afba8d",
   "metadata": {},
   "source": [
    "## 9.8.1 분할 가능한 파일 타입과 압축 방식\n",
    "- 특정 파일 포맷은 기본적으로 분할을 지원\n",
    "- 압축 방식 관리 필요\n",
    "    - 모든 압축 방식이 분할 압축을 지원하지는 않음\n",
    "    - 데이터를 저장하는 방식에 따라 스파크 잡이 원활하게 동작하는 데 막대한 영향을 끼칠 수 있음\n",
    "    - 추천하는 파일 포맷과 압축 방식은 파케이 파일 포맷과 GZIP 압축 방식\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b892c54c-e465-448d-9047-5f0e3c645b75",
   "metadata": {},
   "source": [
    "## 9.8.2 병렬로 데이터 읽기\n",
    "- 여러 익스큐터가 같은 파일을 동시에 읽을 수는 없지만 여러 파일을 동시에 읽을 수는 있음\n",
    "    - 다수의 파일이 존재하는 폴더를 읽을 때 폴더의 개별 파일은 DataFrame의 파티션이 됨\n",
    "    - 따라서 사용 가능한 익스큐터를 이용해 병렬(익스큐터 수를 넘어가는 파일은 처리 중인 파일이 완료될 때까지 대기)로 파일을 읽기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a17d98-8801-49c5-a93e-8336449c3397",
   "metadata": {},
   "source": [
    "## 9.8.3 병렬로 데이터 쓰기\n",
    "- 파일이나 데이터 수는 데이터를 쓰는 시점에 DataFrame이 가진 파티션 수에 따라 달라질 수 있음\n",
    "- 기본적으로 데이터 파티션 당 하나의 파일이 작성됨\n",
    "    - 따라서 옵션에 지정된 파일명은 실제로는 다수의 파일을 가진 디렉터리\n",
    "    - 그리고 디렉터리 안에 파티션당 하나의 파일로 데이터를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01ecb51b-0082-42cd-be5e-f2a629b5ecf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csvFile.repartition(5).write.format(\"csv\").save(\"/tmp/multiple.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc50ee-674c-4edf-a051-fe629753e173",
   "metadata": {},
   "source": [
    "### 파티셔닝\n",
    "- 어떤 데이터를 어디에 저장할 것인지 제어할 수 있는 기능\n",
    "- 파티셔닝된 디렉터리 또는 테이블에 파일을 쓸 때 디렉터리별로 컬럼 데이터를 인코딩해 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2c4d68f-b3c6-4ca4-bb6d-57b08d9a6562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csvFile.limit(10).write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\")\n",
    ".save(\"/tmp/partitioned-files.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a351c13-3b75-49d9-8817-1789a52c191a",
   "metadata": {},
   "source": [
    "### 버켓팅\n",
    "- 각 파일에 저장된 데이터를 제어할 수 있는 또 다른 파일 조직화 기법\n",
    "    - 동일한 버킷 ID를 가진 데이터가 하나의 물리적 파티션에 모두 모여 있기 때문에 데이터를 읽을 때 셔플을 피할 수 있음\n",
    "    - 데이터가 이후의 사용 방식에 맞춰 사전에 파티셔닝되므로 조인이나 집계 시 발생하는 고비용을 셔플을 피할 수 있음\n",
    "- 특정 컬럼을 파티셔닝하면 수억 개의 디렉터리를 만들어낼 수도 있음\n",
    "    - 이런 경우 데이터를 버켓팅할 수 있는 방법을 찾아야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65eb5d15-cc7e-41a2-9f95-a6051160bcc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numberBuckets: Int = 10\n",
       "columnToBucketBy: String = count\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numberBuckets = 10\n",
    "val columnToBucketBy = \"count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d9ab673-7c2a-425d-bb34-303733a0966d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csvFile.write.format(\"parquet\").mode(\"overwrite\")\n",
    ".bucketBy(numberBuckets, columnToBucketBy).saveAsTable(\"bucketedFiles\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33308d4a-1317-4a67-aec8-a615e5cd75bc",
   "metadata": {},
   "source": [
    "# Chapter 12. Resilient Distributed Datasets\n",
    "\n",
    "- There are times when higher-level manipulation will not meet the business or engineering problem you are trying to solve.\n",
    "- For those cases, you might need to use Spark's lower-level APIs, specifically RDD, the SparkContext, and distributed shared variables like accumulators and broadcast variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d95ae9-1a97-444a-98f4-400b09a7f224",
   "metadata": {},
   "source": [
    "# What are the Low-Level APIs?\n",
    "- manipulating distributed data(RDDs)\n",
    "- distributing and manipulating distributed ahred variables (broadcast variables and accumulators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bf1d6d-fb06-4a76-b726-4fa651604e5c",
   "metadata": {},
   "source": [
    "## When to Use the Low-Level APIs?\n",
    "- You need some functionality that you cannot find in the higher-level APIs; for example, if you need very tight control over physical data placement across the cluster.\n",
    "- You need to maintain some legacy codebase written using RDDs.\n",
    "- You need to do some custom shared variable manipulation. We will discuss shared variables more in Chapter 14."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2a6da-d71f-421c-b1ec-b247bb2bb8c6",
   "metadata": {},
   "source": [
    "## How to Use the Low-Level APIs?\n",
    "- SparkContext is the entry point for low-level API functionality.\n",
    "    - You can access it through SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b1a05e-219d-4094-9b0f-6442db46a49d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.SparkContext = org.apache.spark.SparkContext@2b86fe63\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b29fc-cdb5-4ecc-a72e-dd280fb85792",
   "metadata": {},
   "source": [
    "# About RDDs\n",
    "- Virtually all Spark code you run, whether DataFrame or Datasets, complies down to an RDD.\n",
    "- RDD represents an immutable, partitioned collection of records that can be operated on in parallel.\n",
    "- In RDDs the records are just Java, Scala, or Python objects of the programmer's choosing\n",
    "    - You can store anything you want in these objects, in any format you want and this gives you great power, but not without potential issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b89816-0373-47e2-81fc-7049c649de92",
   "metadata": {},
   "source": [
    "## Types of RDDs\n",
    "- As a user, you will likely only be creating two types of RDDs: the \"generic\" RDD type or a key-value RDD that provides additional functions, such as aggregating by key.\n",
    "- Internally, each RDD is characterized by five amin properties:\n",
    "    - a list of partitions\n",
    "    - a function for computing each split\n",
    "    - a list of dependencies on other RDDs\n",
    "    - Optionally, a Partitionaer for key-value RDDs(e.g., to say that the RDD is hash-partitioned)\n",
    "    - Optionally, a list of preferred locations on which to compute each split(e.g., block locations for a Hadoop Distributed File System file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd6915e-bf21-47f8-bfb6-b92331cb0e66",
   "metadata": {},
   "source": [
    "- There is no concept of \"rows\" in RDDs; individual records are just raw Java/Scala/Python objects, and you manipulate those manually instead of tapping into the repository of functions that you have in the strucured APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b35cb83-2eae-4f73-b5ac-886f63335f82",
   "metadata": {},
   "source": [
    "- For Scala and Java, the performance is for the most part the same, the large costs incurred in manipulating the raw objects.\n",
    "- Python, however, can lose a substantial amount of performance when using RRDs.\n",
    "    - Running Python RDDs equates to running Python user-defined functions(UDFs) row by row.\n",
    "    - This causes a high overhead for Python RDD manipulations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb208072-e6ad-4ad0-8f57-dfc7f802c327",
   "metadata": {},
   "source": [
    "## When to Use RDDs?\n",
    "- In general, you should not manually create RDDs unless you have a very, very specific reason for doing so.\n",
    "    - For the vast majority of use cases, DataFrames will be more efficient, more stable, and more expressive than RDDs.\n",
    "- The most likely reason for why you'll want to use RDDs is because you need fine-grained control over the physical distribution of data (custom partitioning of data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1429da-c4ef-49b6-a09c-498b599ef40c",
   "metadata": {},
   "source": [
    "## Datasets and RDDs of Case Classes\n",
    "- What is the difference between RDDs of Case Classes and Datasets?\n",
    "    - The difference is that Datasets can still take advantage of the wealth of functions and optimizations that the Structured APIs have to offer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef4a0cb-c079-40a1-b156-3026724aed70",
   "metadata": {},
   "source": [
    "# Creating RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf907f2-bf9c-4f26-a112-30ded6d33d0f",
   "metadata": {},
   "source": [
    "## Interoperating Between DataFrames, Datasets, and RDDs\n",
    "- One of the easiest ways to get RDDs is from an existing DataFrame or Dataset.\n",
    "    - Converting these to an RDD is simple: just use `rdd` method on any of these data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0a7d893-fad8-4098-bee9-3fb1f4329a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: org.apache.spark.rdd.RDD[Long] = MapPartitionsRDD[5] at rdd at <console>:26\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//converts a Dataset[Long] to RDD[Long]\n",
    "spark.range(500).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a864fc17-31e6-4aa7-81dc-685b8d5a2b23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: org.apache.spark.sql.DataFrame = [value: bigint]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//create DataFrame or Dataset from an RDD\n",
    "spark.range(10).rdd.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d2034-6eb6-48c9-9826-edb7351f457b",
   "metadata": {},
   "source": [
    "## From a Local Collection\n",
    "- To create an RDD from a collection, you will need to use the `parallelize` method on a `SparkContext`.\n",
    "    - This turns a single node collection into a parallel collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dbaa9e1-0a8a-4958-9ee6-611982a9fd2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myCollection: Array[String] = Array(Spark, The, Definitive, Guide, :, Big, Data, Processing, Made, Simple)\n",
       "words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[12] at parallelize at <console>:24\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\n",
    "val words = spark.sparkContext.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fcebccb-3f83-4221-8604-08b7125188fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: String = myWords\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.setName(\"myWords\")\n",
    "words.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae847b08-8124-49d3-82d2-351b30d8b7b9",
   "metadata": {},
   "source": [
    "## From Data Sources\n",
    "- It's often preferable to use the Data Sources APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67a7baaa-6e42-4d10-85e4-165b84fecce6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: org.apache.spark.rdd.RDD[String] = /some/path/withTextFiles MapPartitionsRDD[14] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.textFile(\"/some/path/withTextFiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf95ed-31ff-4025-a52e-2bb184d4a5ad",
   "metadata": {},
   "source": [
    "- This creates an RDD for which each record in the RDD represents a line in that text file or files.\n",
    "- Alternatively, you can read in data for which each text file should become a single record.\n",
    "    - The use case here would be where each file is a file that consists of a large JSON object or some document that you will operate on as an individual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a08fe0b1-b455-44f3-92c3-448ffd4d403e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: org.apache.spark.rdd.RDD[(String, String)] = /some/path/withTextFiles MapPartitionsRDD[16] at wholeTextFiles at <console>:25\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.wholeTextFiles(\"/some/path/withTextFiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14599b0d-4239-4b69-bb49-006b1f448615",
   "metadata": {},
   "source": [
    "# Manipulating RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b4870a-f78a-41e5-befb-e1cb71a23700",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "- Just as you do with DataFrames and Datasets, you specify transformations on one RDD to create another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cab456f-0ce6-4ffa-8ca6-52f35d6f85e9",
   "metadata": {},
   "source": [
    "## distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab5bf572-b4b0-48ce-90f2-c27f518530a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Long = 10\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58609c60-bf93-486b-99c0-19a7e39e53a7",
   "metadata": {},
   "source": [
    "## filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d99e41f-c703-40df-9e6b-d54143490e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "startsWithS: (individual: String)Boolean\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def startsWithS(individual:String) ={\n",
    "    individual.startsWith(\"S\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f62c829-6aa8-410b-b91b-d3be88ad9b1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "java.lang.InternalError",
     "evalue": " java.lang.IllegalAccessException: final field has no write access: $Lambda$3732/0x0000000801ff2898.arg$1/putField, from class java.lang.Object (module java.base)",
     "output_type": "error",
     "traceback": [
      "java.lang.InternalError: java.lang.IllegalAccessException: final field has no write access: $Lambda$3732/0x0000000801ff2898.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:167)",
      "  at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:145)",
      "  at java.base/java.lang.reflect.Field.acquireOverrideFieldAccessor(Field.java:1184)",
      "  at java.base/java.lang.reflect.Field.getOverrideFieldAccessor(Field.java:1153)",
      "  at java.base/java.lang.reflect.Field.set(Field.java:820)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:406)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2501)",
      "  at org.apache.spark.rdd.RDD.$anonfun$filter$1(RDD.scala:431)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.RDD.filter(RDD.scala:430)",
      "  ... 34 elided",
      "Caused by: java.lang.IllegalAccessException: final field has no write access: $Lambda$3732/0x0000000801ff2898.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectField(MethodHandles.java:3511)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectSetter(MethodHandles.java:3502)",
      "  at java.base/java.lang.invoke.MethodHandleImpl$1.unreflectField(MethodHandleImpl.java:1630)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:145)",
      "  ... 46 more",
      ""
     ]
    }
   ],
   "source": [
    "words.filter(word => startsWithS(word)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca9284-60ad-49c5-9b80-45b0fb8de2ae",
   "metadata": {},
   "source": [
    "## map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2942525-bdf5-4829-a28e-f10a80241353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words2: org.apache.spark.rdd.RDD[(String, Char, Boolean)] = MapPartitionsRDD[20] at map at <console>:24\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val words2 = words.map(word => (word, word(0), word.startsWith(\"S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da381f2d-e138-4bfe-bf5e-e8cefd6d5c5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Array[(String, Char, Boolean)] = Array((Spark,S,true), (Simple,S,true))\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words2.filter(record => record._3).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e47395f-3a51-4097-aaba-b302b493de21",
   "metadata": {},
   "source": [
    "## flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a28c943c-c2ca-40bd-94c2-6f81c7016657",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Array[Char] = Array(S, p, a, r, k)\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.flatMap(word => word.toSeq).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530d787b-dc3b-4dce-aa96-f764fc4cb0e5",
   "metadata": {},
   "source": [
    "## sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87ceb47b-3868-4820-aabd-a1f5057e1d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: Array[String] = Array(Definitive, Processing)\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.sortBy(word => word.length() * -1).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1dbbe3-cff2-4231-ae89-f4a2985b4708",
   "metadata": {},
   "source": [
    "## Random Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edd925ce-6715-4604-85ec-266445b8230d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fiftyFifytySplit: Array[org.apache.spark.rdd.RDD[String]] = Array(MapPartitionsRDD[28] at randomSplit at <console>:24, MapPartitionsRDD[29] at randomSplit at <console>:24)\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fiftyFifytySplit = words.randomSplit(Array[Double](0.5, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932188c9-70dc-4ddc-b215-f35d48879c14",
   "metadata": {},
   "source": [
    "# Actions\n",
    "- Actions either collect data to the driver or write to an external data source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e23d16-fdc2-46be-b4a8-e7a8353f16d0",
   "metadata": {},
   "source": [
    "## reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2616e9fe-dd86-4897-bb88-b32cc0d37b80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Int = 210\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(1 to 20).reduce(_ + _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05c9f4a1-3bc0-43a0-ac0b-00eb3c0ee97f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "java.lang.InternalError",
     "evalue": " java.lang.IllegalAccessException: final field has no write access: $Lambda$3924/0x0000000802095050.arg$1/putField, from class java.lang.Object (module java.base)",
     "output_type": "error",
     "traceback": [
      "java.lang.InternalError: java.lang.IllegalAccessException: final field has no write access: $Lambda$3924/0x0000000802095050.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:167)",
      "  at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:145)",
      "  at java.base/java.lang.reflect.Field.acquireOverrideFieldAccessor(Field.java:1184)",
      "  at java.base/java.lang.reflect.Field.getOverrideFieldAccessor(Field.java:1153)",
      "  at java.base/java.lang.reflect.Field.set(Field.java:820)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:406)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2501)",
      "  at org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1094)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.RDD.reduce(RDD.scala:1093)",
      "  ... 34 elided",
      "Caused by: java.lang.IllegalAccessException: final field has no write access: $Lambda$3924/0x0000000802095050.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectField(MethodHandles.java:3511)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectSetter(MethodHandles.java:3502)",
      "  at java.base/java.lang.invoke.MethodHandleImpl$1.unreflectField(MethodHandleImpl.java:1630)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:145)",
      "  ... 46 more",
      ""
     ]
    }
   ],
   "source": [
    "def wordLengthReducer(leftWord:String, rightWord:String): String = {\n",
    "    if (leftWord.length > rightWord.length)\n",
    "        return leftWord\n",
    "    else\n",
    "        return rightWord\n",
    "}\n",
    "\n",
    "words.reduce(wordLengthReducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34020353-814f-4ed0-98de-eeb9bfa7e8ef",
   "metadata": {},
   "source": [
    "## count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2dbf6fd0-45a2-4464-aea0-22e233be7a54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Long = 10\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d8913-1975-4c67-83b1-a16be4daad44",
   "metadata": {},
   "source": [
    "## countApprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2914a60a-035b-4c95-8b5f-67401b619a07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confidence: Double = 0.95\n",
       "timeoutMilliseconds: Int = 400\n",
       "res16: org.apache.spark.partial.PartialResult[org.apache.spark.partial.BoundedDouble] = (final: [10.000, 10.000])\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val confidence = 0.95\n",
    "val timeoutMilliseconds = 400\n",
    "words.countApprox(timeoutMilliseconds, confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ba9092-2dc5-420f-ad24-38d4f39d6738",
   "metadata": {},
   "source": [
    "## countApproxDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61ae095a-6f96-4f5b-a531-0e59df6ffeeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Long = 10\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.countApproxDistinct(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4efacba5-4c1c-4e63-8731-575250579f87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Long = 10\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.countApproxDistinct(4, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41e6c26-2515-4c70-8402-c4d5f1b4fa9c",
   "metadata": {},
   "source": [
    "## countByValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dad66101-deed-4f07-a68b-fdd4510339d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: scala.collection.Map[String,Long] = Map(Definitive -> 1, Simple -> 1, Processing -> 1, The -> 1, Spark -> 1, Made -> 1, Guide -> 1, Big -> 1, : -> 1, Data -> 1)\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548bb017-4147-4f2d-9cc8-91bafc9ecc1c",
   "metadata": {},
   "source": [
    "## countByValueApprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c26f277-d4e3-4af8-8d80-9a42186bc0f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res20: org.apache.spark.partial.PartialResult[scala.collection.Map[String,org.apache.spark.partial.BoundedDouble]] = (final: Map(Definitive -> [1.000, 1.000], Simple -> [1.000, 1.000], Processing -> [1.000, 1.000], The -> [1.000, 1.000], Spark -> [1.000, 1.000], Made -> [1.000, 1.000], Guide -> [1.000, 1.000], Big -> [1.000, 1.000], : -> [1.000, 1.000], Data -> [1.000, 1.000]))\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.countByValueApprox(1000, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62849f58-d540-40be-8902-9354143fe13b",
   "metadata": {},
   "source": [
    "## first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c942e87-691e-4b63-a26a-5de5b282e4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: Int = 1\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(1 to 20).max()\n",
    "spark.sparkContext.parallelize(1 to 20).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0ea35-e9dc-4d84-8f34-9c1c091fee8a",
   "metadata": {},
   "source": [
    "## take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa00082c-2ffc-4973-b3cb-d41204839371",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "withReplacement: Boolean = true\n",
       "numberToTake: Int = 6\n",
       "randomSeed: Long = 100\n",
       "res22: Array[String] = Array(Guide, Spark, :, Simple, Simple, Spark)\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(5)\n",
    "words.takeOrdered(5)\n",
    "words.top(5)\n",
    "val withReplacement = true\n",
    "val numberToTake = 6\n",
    "val randomSeed = 100L\n",
    "words.takeSample(withReplacement, numberToTake, randomSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f33e47b-ee95-4651-8fc4-7fafbae0ddb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res23: Array[String] = Array(Spark, The, Definitive, Guide, :)\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d94c1c5e-2251-4c15-a15e-394e87096b46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res24: Array[String] = Array(:, Big, Data, Definitive, Guide)\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.takeOrdered(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35a37606-e170-46a3-a918-7dbcba9c3df6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res25: Array[String] = Array(The, Spark, Simple, Processing, Made)\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.top(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed87c57-09b2-47c1-ac8d-f1f788832fa4",
   "metadata": {},
   "source": [
    "# Saving Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60a0fa-a359-4d02-a2b1-38f65796a679",
   "metadata": {
    "tags": []
   },
   "source": [
    "- With RDDs, you cannot actually \"save\" to a data source in the conventional sense.\n",
    "    - You must iterate over the partitions in order to save the contents of each partition to some external database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a1ed17-02c6-4cbe-bf9b-d455437808fa",
   "metadata": {},
   "source": [
    "## saveAsTextFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2561cba-1251-420f-b798-925ed5371f44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "org.apache.hadoop.mapred.FileAlreadyExistsException",
     "evalue": " Output directory file:/tmp/bookTitle already exists",
     "output_type": "error",
     "traceback": [
      "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/tmp/bookTitle already exists",
      "  at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)",
      "  at org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)",
      "  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)",
      "  at org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)",
      "  at org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)",
      "  ... 34 elided",
      ""
     ]
    }
   ],
   "source": [
    "words.saveAsTextFile(\"file:/tmp/bookTitle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05189f46-8543-4911-b47d-19ee9fa45050",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "org.apache.hadoop.mapred.FileAlreadyExistsException",
     "evalue": " Output directory file:/tmp/bookTitleCompressed already exists",
     "output_type": "error",
     "traceback": [
      "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/tmp/bookTitleCompressed already exists",
      "  at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)",
      "  at org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)",
      "  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)",
      "  at org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)",
      "  ... 34 elided",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.hadoop.io.compress.BZip2Codec\n",
    "words.saveAsTextFile(\"file:/tmp/bookTitleCompressed\", classOf[BZip2Codec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30de8f71-74a8-4b0a-850e-121c0f5d40fa",
   "metadata": {},
   "source": [
    "## SequenceFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c037d7b7-ae8d-4c97-8f8b-24366962700b",
   "metadata": {},
   "source": [
    "- A `sequenceFile` is a flat file consisting of binary key-value pairs.\n",
    "- It is extensively used in MapReduce as input/output formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6b09c6d-c710-4cdc-a5d1-24b8ee12c4c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "org.apache.hadoop.mapred.FileAlreadyExistsException",
     "evalue": " Output directory file:/tmp/my/sequenceFilePath already exists",
     "output_type": "error",
     "traceback": [
      "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/tmp/my/sequenceFilePath already exists",
      "  at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)",
      "  at org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)",
      "  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)",
      "  at org.apache.spark.rdd.SequenceFileRDDFunctions.$anonfun$saveAsSequenceFile$1(SequenceFileRDDFunctions.scala:66)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.SequenceFileRDDFunctions.saveAsSequenceFile(SequenceFileRDDFunctions.scala:51)",
      "  at org.apache.spark.rdd.RDD.$anonfun$saveAsObjectFile$1(RDD.scala:1608)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.RDD.saveAsObjectFile(RDD.scala:1608)",
      "  ... 34 elided",
      ""
     ]
    }
   ],
   "source": [
    "words.saveAsObjectFile(\"/tmp/my/sequenceFilePath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077226f-032b-44e7-a1f1-223c9a58d52a",
   "metadata": {},
   "source": [
    "## Hadoop Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2006d39a-eb46-488a-afd0-d1c249de1ecd",
   "metadata": {},
   "source": [
    "# Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfd6637f-3500-4413-85e9-44fc9e09f866",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res29: words.type = myWords ParallelCollectionRDD[12] at parallelize at <console>:24\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b303674-caec-46ae-a536-38f515499547",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res30: org.apache.spark.storage.StorageLevel = StorageLevel(memory, deserialized, 1 replicas)\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.getStorageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f89947-3238-45ef-9af2-7e38098e61f7",
   "metadata": {},
   "source": [
    "# Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3764ec3-8b25-413d-8b27-f4045d77d61e",
   "metadata": {},
   "source": [
    "- Checkpointing is the act of saving an RDD to disk so that future references to this RDD point to those intermediate partitions on disk rather than recomputing the RDD from its original source.\n",
    "- This is similar to caching except that it's not stored in memory, only disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7da31416-5c4d-4b08-ad1a-a6841a368604",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "java.io.FileNotFoundException",
     "evalue": " File /some/path/for/checkpointing/b8240637-20c3-4744-bd76-f6a3a4aef510 does not exist",
     "output_type": "error",
     "traceback": [
      "java.io.FileNotFoundException: File /some/path/for/checkpointing/b8240637-20c3-4744-bd76-f6a3a4aef510 does not exist",
      "  at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)",
      "  at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)",
      "  at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)",
      "  at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)",
      "  at org.apache.spark.SparkContext.$anonfun$setCheckpointDir$2(SparkContext.scala:2526)",
      "  at scala.Option.map(Option.scala:230)",
      "  at org.apache.spark.SparkContext.setCheckpointDir(SparkContext.scala:2522)",
      "  ... 34 elided",
      ""
     ]
    }
   ],
   "source": [
    "spark.sparkContext.setCheckpointDir(\"/some/path/for/checkpointing\")\n",
    "words.checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10eb0be-2666-40be-9e3f-acef73a57c07",
   "metadata": {},
   "source": [
    "# Pipe RDDs to System Commands\n",
    "- The `pipe` method is probably one of Spark's more interesting methods.\n",
    "- With pipe, you can return an RDD created by piping elements to a forked external process.\n",
    "    - The resulting RDD is computed by executing the given process once per partition.\n",
    "    - All elements of each input partition are written to a process's stdin as lines of input separated by a newline.\n",
    "    - The resulting partition consists of the precess's stdout output, with each line of stdout resulting in one element of the output partition.\n",
    "    - A process is invoked even for empty partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2aa49515-b772-4e1a-9a47-cebd5b407391",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res32: Array[String] = Array(\"       5\", \"       5\")\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.pipe(\"wc -l\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c8763-2c2d-47e6-b0cd-1eab99731709",
   "metadata": {},
   "source": [
    "## mapPartitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1a535e0-88fc-4c23-9c1f-9ca7df2c9422",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res33: Double = 2.0\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.mapPartitions(part => Iterator[Int](1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9fca46d8-4885-4dbc-8fb8-bfa078d80a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "java.lang.InternalError",
     "evalue": " java.lang.IllegalAccessException: final field has no write access: $Lambda$4253/0x000000080216ce48.arg$1/putField, from class java.lang.Object (module java.base)",
     "output_type": "error",
     "traceback": [
      "java.lang.InternalError: java.lang.IllegalAccessException: final field has no write access: $Lambda$4253/0x000000080216ce48.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:167)",
      "  at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:145)",
      "  at java.base/java.lang.reflect.Field.acquireOverrideFieldAccessor(Field.java:1184)",
      "  at java.base/java.lang.reflect.Field.getOverrideFieldAccessor(Field.java:1153)",
      "  at java.base/java.lang.reflect.Field.set(Field.java:820)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:406)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2501)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$1(RDD.scala:904)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.RDD.mapPartitionsWithIndex(RDD.scala:903)",
      "  ... 34 elided",
      "Caused by: java.lang.IllegalAccessException: final field has no write access: $Lambda$4253/0x000000080216ce48.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectField(MethodHandles.java:3511)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectSetter(MethodHandles.java:3502)",
      "  at java.base/java.lang.invoke.MethodHandleImpl$1.unreflectField(MethodHandleImpl.java:1630)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:145)",
      "  ... 46 more",
      ""
     ]
    }
   ],
   "source": [
    "def indexedFunc(partitionIndex:Int, withinPartIterator: Iterator[String]) = {\n",
    "    withinPartIterator.toList.map(\n",
    "        value => s\"Partition: $partitionIndex => $value\").iterator\n",
    "}\n",
    "words.mapPartitionsWithIndex(indexedFunc).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359304f6-7f2b-48dc-a6ba-978ee1bd0272",
   "metadata": {},
   "source": [
    "## foreachPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd66cec1-75b6-40ff-ab87-dea707a6772d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words.foreachPartition { iter =>\n",
    "    import java.io._\n",
    "    import scala.util.Random\n",
    "    val randomFileName = new Random().nextInt()\n",
    "    val pw = new PrintWriter(new File(s\"/tmp/random-file-${randomFileName}.txt\"))\n",
    "    while (iter.hasNext) {\n",
    "        pw.write(iter.next())\n",
    "    }\n",
    "    pw.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c84bdf-2b03-456b-a3b1-40a2e65c899a",
   "metadata": {},
   "source": [
    "## glom\n",
    "- takes every partition in your dataset and converts them to arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e51063ef-c469-4339-8873-bd116f2b59d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res36: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[49] at glom at <console>:25\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(Seq(\"Hello\", \"World\"), 2).glom()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

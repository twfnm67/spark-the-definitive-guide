{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a282869-d088-4dba-8b20-bb2d6377102a",
   "metadata": {},
   "source": [
    "# 6.2 스파크 데이터 타입으로 변환하기\n",
    "- lit 함수를 사용\n",
    "- lit 함수 : 다른 언어의 데이터 타입을 스파크 데이터 타입에 맞게 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f0f7b9-11a7-44a0-9912-2f5963c88afe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.2:4040\n",
       "SparkContext available as 'sc' (version = 3.3.2, master = local[*], app id = local-1681990673327)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5450c6e8\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3af58f48-07f7-486e-a394-f23774d58bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.lit\n",
       "df: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n",
       "res1: org.apache.spark.sql.DataFrame = [5: int, five: string ... 1 more field]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.lit\n",
    "\n",
    "val df = spark.read.format(\"csv\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".load(\"sample_data/retail-data/by-day/2010-12-01.csv\")\n",
    "\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710dfaf4-fc0b-408b-a99b-aa6527d6353d",
   "metadata": {},
   "source": [
    "# 6.3 불리언 데이터 타입 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e27e45b-b8d5-4789-83fd-946a510c84fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------------+\n",
      "|InvoiceNo|Description                        |\n",
      "+---------+-----------------------------------+\n",
      "|536365   |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|536365   |WHITE METAL LANTERN                |\n",
      "|536365   |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|536365   |KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|536365   |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+---------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.col\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "df.where(col(\"InvoiceNo\").equalTo(536365)).select(\"InvoiceNo\", \"Description\").show(5,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcba3c63-d6aa-45f8-a030-f56924508f88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------------+\n",
      "|InvoiceNo|Description                        |\n",
      "+---------+-----------------------------------+\n",
      "|536365   |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|536365   |WHITE METAL LANTERN                |\n",
      "|536365   |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|536365   |KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|536365   |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+---------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.col\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "df.where(col(\"InvoiceNo\") === 536365)\n",
    ".select(\"InvoiceNo\", \"Description\")\n",
    ".show(5,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82859392-80d4-45fc-becd-465b2f049ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "priceFilter: org.apache.spark.sql.Column = (UnitPrice > 600)\n",
       "descripFilter: org.apache.spark.sql.Column = contains(Description, POSTAGE)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//필터링 조건에 불리언 표현식 사용\n",
    "val priceFilter = col(\"UnitPrice\") >600\n",
    "val descripFilter = col(\"Description\").contains(\"POSTAGE\")\n",
    "\n",
    "df.where(col(\"StockCode\").isin(\"DOT\")).where(priceFilter.or(descripFilter)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f85e67d3-ca73-4c71-ad3e-2e08419a7385",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|unitPrice|isExpensive|\n",
      "+---------+-----------+\n",
      "|   569.77|       true|\n",
      "|   607.49|       true|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DOTCodeFilter: org.apache.spark.sql.Column = (StockCode = DOT)\n",
       "priceFilter: org.apache.spark.sql.Column = (UnitPrice > 600)\n",
       "descripFilter: org.apache.spark.sql.Column = contains(Description, POSTAGE)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//불리언 컬럼을 사용해 DataFrame을 필터링\n",
    "val DOTCodeFilter = col(\"StockCode\") === \"DOT\"\n",
    "val priceFilter = col(\"UnitPrice\") > 600\n",
    "val descripFilter = col(\"Description\").contains(\"POSTAGE\")\n",
    "\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter.and(priceFilter.or(descripFilter)))\n",
    ".where(\"isExpensive\")\n",
    ".select(\"unitPrice\", \"isExpensive\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791c66c0-0a58-468b-aee5-880e6314e88c",
   "metadata": {},
   "source": [
    "# 6.4 수치형 데이터 타입 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c21b6832-01f2-4356-835c-7a59ab9e4d1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "|   17850.0|             489.0|\n",
      "|   17850.0|          418.7156|\n",
      "|   17850.0|          418.7156|\n",
      "|   17850.0|239.09000000000003|\n",
      "|   17850.0|            655.25|\n",
      "|   17850.0|128.21000000000004|\n",
      "|   17850.0|128.21000000000004|\n",
      "|   13047.0|2929.6463999999996|\n",
      "|   13047.0|163.76000000000005|\n",
      "|   13047.0|163.76000000000005|\n",
      "|   13047.0|             905.0|\n",
      "|   13047.0|103.00999999999998|\n",
      "|   13047.0|            655.25|\n",
      "|   13047.0|225.52250000000004|\n",
      "|   13047.0|401.00999999999993|\n",
      "|   13047.0|323.62250000000006|\n",
      "|   13047.0|323.62250000000006|\n",
      "|   13047.0|           1016.24|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{expr, pow}\n",
       "fabricatedQuantity: org.apache.spark.sql.Column = (POWER((Quantity * UnitPrice), 2.0) + 5)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{expr, pow}\n",
    "\n",
    "//pow\n",
    "val fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4a99dce-a8a4-43a2-ba90-53a566d7be12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|rounded|UnitPrice|\n",
      "+-------+---------+\n",
      "|    2.6|     2.55|\n",
      "|    3.4|     3.39|\n",
      "|    2.8|     2.75|\n",
      "|    3.4|     3.39|\n",
      "|    3.4|     3.39|\n",
      "+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{round, bround}\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//반올림 round, 내림 bround\n",
    "import org.apache.spark.sql.functions.{round, bround}\n",
    "\n",
    "df.select(round(col(\"UnitPrice\"), 1).alias(\"rounded\"), col(\"UnitPrice\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48c37da6-8364-424e-977b-6f5b8c584705",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|corr(Quantity, UnitPrice)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835551|\n",
      "+-------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.corr\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//피어슨 상관계수\n",
    "import org.apache.spark.sql.functions.{corr}\n",
    "\n",
    "df.stat.corr(\"Quantity\", \"UnitPrice\")\n",
    "df.select(corr(\"Quantity\", \"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5999369d-1952-4945-b53d-2b041cc9606e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128| 4.151946589446603|15661.388719512195|          null|\n",
      "| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|15.638659854603892|1854.4496996893627|          null|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//요약 통계\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02180836-ac26-4cd1-b795-71b435902ab4",
   "metadata": {},
   "source": [
    "# 6.5 문자열 데이터  타입 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07e66abb-a832-4b53-a43a-ebe3d483e9fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|initcap(Description)              |\n",
      "+----------------------------------+\n",
      "|White Hanging Heart T-light Holder|\n",
      "|White Metal Lantern               |\n",
      "+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.initcap\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//대소문자 변환\n",
    "//initcap 주어진 문자열에서 공백으로 나뉘는 모든 단어의 첫 글자를 대문자로 변경\n",
    "import org.apache.spark.sql.functions.{initcap}\n",
    "\n",
    "df.select(initcap(col(\"Description\"))).show(2, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4ebcd8f-b72b-4301-8cfe-520b187acef1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------------+\n",
      "|         Description|  lower(Description)|upper(lower(Description))|\n",
      "+--------------------+--------------------+-------------------------+\n",
      "|WHITE HANGING HEA...|white hanging hea...|     WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN| white metal lantern|      WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{lower, upper}\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//lower, upper\n",
    "import org.apache.spark.sql.functions.{lower, upper}\n",
    "\n",
    "df.select(col(\"Description\"),\n",
    "          lower(col(\"Description\")),\n",
    "          upper(lower(col(\"Description\")))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092cfd25-cd5b-4814-8796-79b0bc54ad59",
   "metadata": {},
   "source": [
    "## 6.5.1 정규 표현식\n",
    "- 정규 표현식을 사용해 문자열에서 값을 추출하거나 다른 값으로 치환하는 데 필요한 규칙 모음을 정의할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5999d0c-91f5-4f02-97f7-7cb9417fdf02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         color_clean|         Description|\n",
      "+--------------------+--------------------+\n",
      "|COLOR HANGING HEA...|WHITE HANGING HEA...|\n",
      "| COLOR METAL LANTERN| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.regexp_replace\n",
       "simpleColors: Seq[String] = List(black, white, red, green, blue)\n",
       "regexString: String = BLACK|WHITE|RED|GREEN|BLUE\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.regexp_replace\n",
    "\n",
    "val simpleColors = Seq(\"black\", \"white\", \"red\", \"green\", \"blue\")\n",
    "val regexString = simpleColors.map(_.toUpperCase).mkString(\"|\")\n",
    "\n",
    "df.select(\n",
    "    regexp_replace(col(\"Description\"), regexString, \"COLOR\").alias(\"color_clean\"),\n",
    "    col(\"Description\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e289c03-4cac-41e5-aa30-8300c22d9a47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------------------+\n",
      "|translate(Description, LEET, 1337)|         Description|\n",
      "+----------------------------------+--------------------+\n",
      "|              WHI73 HANGING H3A...|WHITE HANGING HEA...|\n",
      "|               WHI73 M37A1 1AN73RN| WHITE METAL LANTERN|\n",
      "+----------------------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.translate\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.translate\n",
    "\n",
    "df.select(translate(col(\"Description\"), \"LEET\", \"1337\"), col(\"Description\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dd756e-8d88-4ae6-ba28-e6122efe27bf",
   "metadata": {},
   "source": [
    "# 6.6 날짜와 타임스탬프 데이터 타입 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f600ff79-96c0-4097-8b76-ca6683a51d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{current_date, current_timestamp}\n",
       "dateDF: org.apache.spark.sql.DataFrame = [id: bigint, today: date ... 1 more field]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//오늘 날짜와 현재 타임스탬프 값 구하기\n",
    "import org.apache.spark.sql.functions.{current_date, current_timestamp}\n",
    "\n",
    "val dateDF = spark.range(10)\n",
    ".withColumn(\"today\", current_date())\n",
    ".withColumn(\"now\", current_timestamp())\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be4acc1f-d455-4a78-83d6-b3aae8540a91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2023-04-15|        2023-04-25|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{date_add, date_sub}\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//위 DataFrame을 사용해 오늘을 기준으로 5일 전후의 날짜 구하기\n",
    "//date_sub, date_add\n",
    "import org.apache.spark.sql.functions.{date_add, date_sub}\n",
    "\n",
    "dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a935257-f902-4d8c-9244-a7abb7f77fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{datediff, months_between, to_date}\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//두 날짜 사이의 일 수, 두 날짜 사이의 개월 수\n",
    "import org.apache.spark.sql.functions.{datediff, months_between, to_date}\n",
    "\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\n",
    ".select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e30e0d8-2700-456a-b83c-60c5abf6cb38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                    -16.67741935|\n",
      "+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.select(\n",
    "    to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "    to_date(lit(\"2017-05-22\")).alias(\"end\"))\n",
    "        .select(months_between(col(\"start\"), col(\"end\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10fe2300-968f-44d2-a896-e0dc5fe68cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.to_date\n",
       "dateFormat: String = yyyy-dd-MM\n",
       "cleanDateDF: org.apache.spark.sql.DataFrame = [date: date, date2: date]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//날짜 포맷 지정\n",
    "import org.apache.spark.sql.functions.to_date\n",
    "\n",
    "val dateFormat = \"yyyy-dd-MM\"\n",
    "val cleanDateDF = spark.range(1).select(\n",
    "    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n",
    "    to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23144c1c-2690-4642-a397-84fb673d3ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|to_timestamp(date, yyyy-dd-MM)|\n",
      "+------------------------------+\n",
      "|           2017-11-12 00:00:00|\n",
      "+------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.to_timestamp\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.to_timestamp\n",
    "\n",
    "cleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50bec8d-0cf1-43c4-a160-db71fe33e582",
   "metadata": {},
   "source": [
    "# 6.7 null 값 다루기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3894036-07fa-47e6-b75c-af8aa39ad1f6",
   "metadata": {},
   "source": [
    "# 6.9 복합 데이터 타입 다루기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f58b85-4d08-4f66-98b7-4c2494930939",
   "metadata": {},
   "source": [
    "## 6.9.1 구조체\n",
    "- DataFrame 내부의 DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34900afe-fe75-4755-a9dd-c36306b859d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.struct\n",
       "complexDF: org.apache.spark.sql.DataFrame = [complex: struct<Description: string, InvoiceNo: string>]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//df.selectExpr(\"(Description, InvoiceNo) as complex\", \"\\*\")\n",
    "//df.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"\\*\")\n",
    "\n",
    "import org.apache.spark.sql.functions.struct\n",
    "\n",
    "val complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf9b3ee-0a1b-4366-b089-729ab56152dc",
   "metadata": {},
   "source": [
    "## 6.9.2 배열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d039ebab-1045-4830-a813-6784e8b7bd2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|split(Description,  , -1)|\n",
      "+-------------------------+\n",
      "|     [WHITE, HANGING, ...|\n",
      "|     [WHITE, METAL, LA...|\n",
      "+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.split\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//split 함수를 통해 Description 컬럼을 배열로 변환\n",
    "import org.apache.spark.sql.functions.split\n",
    "\n",
    "df.select(split(col(\"Description\"), \" \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b26ceae6-5bf3-44a8-b64b-934539a3883c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\")).selectExpr(\"array_col[0]\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627f5755-d2c9-4dac-9bf6-666849185510",
   "metadata": {},
   "source": [
    "### 배열의 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9d201b5-bad0-4a5a-9158-c97157d0f272",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|size(split(Description,  , -1))|\n",
      "+-------------------------------+\n",
      "|                              5|\n",
      "|                              3|\n",
      "+-------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.size\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.size\n",
    "\n",
    "df.select(size(split(col(\"Description\"), \" \"))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13af45-23d4-464e-a490-0121a1fe1f4f",
   "metadata": {},
   "source": [
    "### array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecd348f7-7b9a-4a16-857b-5a3c61b03403",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|array_contains(split(Description,  , -1), WHITE)|\n",
      "+------------------------------------------------+\n",
      "|                                            true|\n",
      "|                                            true|\n",
      "+------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.array_contains\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.array_contains\n",
    "\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1a005b-b591-4388-84c1-89c03ed4ac5c",
   "metadata": {},
   "source": [
    "### explode\n",
    "- 배열 타입의 컬럼을 입력받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44b6c8ab-0e26-484d-9574-77f996d5cca3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------+\n",
      "|         Description|InvoiceNo|exploded|\n",
      "+--------------------+---------+--------+\n",
      "|WHITE HANGING HEA...|   536365|   WHITE|\n",
      "|WHITE HANGING HEA...|   536365| HANGING|\n",
      "+--------------------+---------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{split, explode}\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{split, explode}\n",
    "\n",
    "df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\n",
    ".withColumn(\"exploded\", explode(col(\"splitted\")))\n",
    ".select(\"Description\", \"InvoiceNo\", \"exploded\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e42699-97e5-4765-9e4a-85003dc65e5f",
   "metadata": {},
   "source": [
    "## 6.9.3 맵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1810a2eb-9f15-4d4f-b024-8c6feb33afda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         complex_map|\n",
      "+--------------------+\n",
      "|{WHITE HANGING HE...|\n",
      "|{WHITE METAL LANT...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.map\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.map\n",
    "\n",
    "df.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd2afeea-7318-42f9-81f5-cc15ced6fe0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|                            null|\n",
      "|                          536365|\n",
      "+--------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//적합한 키를 사용해 데이터 조회\n",
    "df.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\n",
    ".selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c86c3-d12e-40ca-b5b8-62d33fe353be",
   "metadata": {},
   "source": [
    "# 6.10 JSON 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f844f16-fcf0-4678-acda-fa4072a6848f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jsonDF: org.apache.spark.sql.DataFrame = [jsonString: string]\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//JSON 컬럼 생성\n",
    "val jsonDF = spark.range(1).selectExpr(\"\"\"\n",
    "'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f161396-48ff-48b1-a000-062edfeb5f38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|column|                  c0|\n",
      "+------+--------------------+\n",
      "|     2|{\"myJSONValue\":[1...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{get_json_object, json_tuple}\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//get_json_object 함수로 JSON 객체를 인라인 쿼리로 조회\n",
    "import org.apache.spark.sql.functions.{get_json_object, json_tuple}\n",
    "\n",
    "jsonDF.select(\n",
    "    get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\") as \"column\",\n",
    "    json_tuple(col(\"jsonString\"), \"myJSONKey\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3c8b314-f3c9-4eb9-8649-c07bedbf42b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.to_json\n",
       "res33: org.apache.spark.sql.DataFrame = [to_json(myStruct): string]\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//StructType을 JSON 문자열로 변경\n",
    "import org.apache.spark.sql.functions.to_json\n",
    "\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\n",
    ".select(to_json(col(\"myStruct\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e9e9e9-f9b7-4603-abaf-5c531411c7b4",
   "metadata": {},
   "source": [
    "# 6.11 사용자 정의 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64517ccf-b83a-4ff2-92e7-3882b189f167",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "udfExampleDF: org.apache.spark.sql.DataFrame = [num: bigint]\n",
       "power3: (number: Double)Double\n",
       "res34: Double = 8.0\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "def power3(number:Double):Double = number * number * number\n",
    "power3(2.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

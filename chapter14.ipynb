{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b895f4-c477-41f0-a3e4-85414c3253e4",
   "metadata": {},
   "source": [
    "# Chapter 14. 분산형 공유 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52728c-9a22-43bf-918e-7256dd47a748",
   "metadata": {},
   "source": [
    "- 스파크의 저수준 API에는 RDD 인터페이스 외에도 두 번째 유형인 '분산형 공유 변수'가 있음\n",
    "    - 브로드캐스트 변수 : 모든 워커 노드에 큰 값을 저장하므로 재전송 없이 많은 스파크 액션에서 재사용할 수 있음\n",
    "    - 어큐뮬레이터 : 모든 태스크의 데이터를 공유 결과에 추가할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb7ae6-b4fa-47c2-8ae0-988672c41623",
   "metadata": {},
   "source": [
    "# 14.1 브로드캐스트 변수\n",
    "- 변하지 않는 값(불변성 값)을 클로저 함수의 변수로 캡슐화하지 않고 클러스터에서 효율적으로 공유하는 방법 제공\n",
    "- 모든 태스크마다 직렬화하지 않고 클러스터의 모든 머신에 캐시하는 불변성 공유 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1721f1fa-8aff-4c79-929d-140d30f5443a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.2:4040\n",
       "SparkContext available as 'sc' (version = 3.3.2, master = local[*], app id = local-1686177429554)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3aaf14d6\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7fb19f3-fccc-4dce-94e0-31e64e9be8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myCollection: Array[String] = Array(Spark, The, Definitive, Guide, :, Big, Data, Processing, Made, Simple)\n",
       "words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:24\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\n",
    "val words = spark.sparkContext.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "096ab361-056a-4104-938e-28b62495c4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "supplementalData: scala.collection.immutable.Map[String,Int] = Map(Spark -> 1000, Definitive -> 200, Big -> -300, Simple -> 100)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val supplementalData = Map(\"Spark\" -> 1000, \"Definitive\" -> 200, \"Big\" -> -300, \"Simple\" -> 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc5d2852-0655-4928-9788-2a3af7a6e3fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "suppBroadcast: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[String,Int]] = Broadcast(0)\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val suppBroadcast = spark.sparkContext.broadcast(supplementalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21678002-9bcc-431f-81f0-65b5e8bc50ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: scala.collection.immutable.Map[String,Int] = Map(Spark -> 1000, Definitive -> 200, Big -> -300, Simple -> 100)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suppBroadcast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e0b5be-5d8d-4594-9fc5-2508b0aa0245",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "java.lang.InternalError",
     "evalue": " java.lang.IllegalAccessException: final field has no write access: $Lambda$2604/0x0000000801aed050.arg$1/putField, from class java.lang.Object (module java.base)",
     "output_type": "error",
     "traceback": [
      "java.lang.InternalError: java.lang.IllegalAccessException: final field has no write access: $Lambda$2604/0x0000000801aed050.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:167)",
      "  at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:145)",
      "  at java.base/java.lang.reflect.Field.acquireOverrideFieldAccessor(Field.java:1184)",
      "  at java.base/java.lang.reflect.Field.getOverrideFieldAccessor(Field.java:1153)",
      "  at java.base/java.lang.reflect.Field.set(Field.java:820)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:406)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2501)",
      "  at org.apache.spark.rdd.RDD.$anonfun$map$1(RDD.scala:414)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.RDD.map(RDD.scala:413)",
      "  ... 34 elided",
      "Caused by: java.lang.IllegalAccessException: final field has no write access: $Lambda$2604/0x0000000801aed050.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectField(MethodHandles.java:3511)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectSetter(MethodHandles.java:3502)",
      "  at java.base/java.lang.invoke.MethodHandleImpl$1.unreflectField(MethodHandleImpl.java:1630)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:145)",
      "  ... 46 more",
      ""
     ]
    }
   ],
   "source": [
    "words.map(word => (word, suppBroadcast.value.getOrElse(word, 0))).sortBy(wordPair => wordPair._2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8987877-4c4a-48d5-b1e1-003697a4c5e8",
   "metadata": {},
   "source": [
    "# 14.2 어큐뮬레이터\n",
    "- 트랜스포메이션 내부의 다양한 값을 갱신하는 데 사용\n",
    "- 스파크 클러스터에서 로우 단위로 안전하게 값을 갱신할 수 있는 변경 가능한 변수를 제공\n",
    "- 어큐뮬레이터의 값은 액션을 처리하는 과정에서만 갱신되며 각 태스크엣 한 번만 갱신하도록 제어됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34339b-a189-46c9-9495-b09a5da4310a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 14.2.1 기본 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc810ba0-1f19-4ce3-a060-b8583122dd16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Flight\n",
       "flights: org.apache.spark.sql.Dataset[Flight] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String, count: BigInt)\n",
    "val flights = spark.read.parquet(\"./flight-data/parquet/2010-summary.parquet\").as[Flight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d12cdb2-14ac-49a4-9eae-7c7b90525912",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.util.LongAccumulator\n",
       "accUnnamed: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 26, name: None, value: 0)\n",
       "acc: Unit = ()\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.util.LongAccumulator\n",
    "\n",
    "val accUnnamed = new LongAccumulator\n",
    "val acc = spark.sparkContext.register(accUnnamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf271755-fa8d-4d80-9bf4-a5f6123fd81b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accChina: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 28, name: Some(China), value: 0)\n",
       "accChina2: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 27, name: Some(China), value: 0)\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accChina = new LongAccumulator\n",
    "val accChina2 = spark.sparkContext.longAccumulator(\"China\")\n",
    "\n",
    "spark.sparkContext.register(accChina, \"China\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "024ed13f-8777-40d6-a9c1-b8c4a1752876",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accChinaFunc: (flight_row: Flight)Unit\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accChinaFunc(flight_row: Flight) = {\n",
    "    val destination = flight_row.DEST_COUNTRY_NAME\n",
    "    val origin = flight_row.ORIGIN_COUNTRY_NAME\n",
    "    \n",
    "    if(destination == \"China\") {\n",
    "        accChina.add(flight_row.count.toLong)\n",
    "    }\n",
    "    if(origin == \"China\") {\n",
    "        accChina.add(flight_row.count.toLong)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c400ae9d-90b0-428c-982e-85bb47d6d2d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "java.lang.InternalError",
     "evalue": " java.lang.IllegalAccessException: final field has no write access: $Lambda$3763/0x0000000801171400.arg$1/putField, from class java.lang.Object (module java.base)",
     "output_type": "error",
     "traceback": [
      "java.lang.InternalError: java.lang.IllegalAccessException: final field has no write access: $Lambda$3763/0x0000000801171400.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:167)",
      "  at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:145)",
      "  at java.base/java.lang.reflect.Field.acquireOverrideFieldAccessor(Field.java:1184)",
      "  at java.base/java.lang.reflect.Field.getOverrideFieldAccessor(Field.java:1153)",
      "  at java.base/java.lang.reflect.Field.set(Field.java:820)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:406)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2501)",
      "  at org.apache.spark.rdd.RDD.$anonfun$foreach$1(RDD.scala:1002)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.RDD.foreach(RDD.scala:1001)",
      "  at org.apache.spark.sql.Dataset.$anonfun$foreach$1(Dataset.scala:3042)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:3845)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:3843)",
      "  at org.apache.spark.sql.Dataset.foreach(Dataset.scala:3042)",
      "  ... 34 elided",
      "Caused by: java.lang.IllegalAccessException: final field has no write access: $Lambda$3763/0x0000000801171400.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectField(MethodHandles.java:3511)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectSetter(MethodHandles.java:3502)",
      "  at java.base/java.lang.invoke.MethodHandleImpl$1.unreflectField(MethodHandleImpl.java:1630)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:145)",
      "  ... 56 more",
      ""
     ]
    }
   ],
   "source": [
    "flights.foreach(flight_row => accChinaFunc(flight_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c8f0d41-ebe6-4744-84e3-b42d2a5b226e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Long = 0\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accChina.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d095fdc-36e6-4bb2-b15b-f80c48df48a9",
   "metadata": {},
   "source": [
    "## 14.4.2 사용자 정의 어큐뮬레이터\n",
    "- 직접 정의하려면 AccumulatorV2 클래스를 상속받아야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba6bb669-721f-40c5-bc70-f8fd83851372",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.collection.mutable.ArrayBuffer\n",
       "import org.apache.spark.util.AccumulatorV2\n",
       "arr: scala.collection.mutable.ArrayBuffer[BigInt] = ArrayBuffer()\n",
       "defined class EvenAccumulator\n",
       "acc: EvenAccumulator = EvenAccumulator(id: 37, name: Some(evenAcc), value: 0)\n",
       "newAcc: Unit = ()\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//짝수값만 더하는 예제\n",
    "\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import org.apache.spark.util.AccumulatorV2\n",
    "\n",
    "val arr = ArrayBuffer[BigInt]()\n",
    "\n",
    "class EvenAccumulator extends AccumulatorV2[BigInt, BigInt] {\n",
    "    private var num:BigInt = 0\n",
    "    def reset(): Unit = {\n",
    "        this.num=0\n",
    "    }\n",
    "    def add(intValue: BigInt): Unit = {\n",
    "        if(intValue % 2 == 0) {\n",
    "            this.num+=intValue\n",
    "        }\n",
    "    }\n",
    "    def merge(other: AccumulatorV2[BigInt, BigInt]): Unit = {\n",
    "        this.num+=other.value\n",
    "    }\n",
    "    def value():BigInt = {\n",
    "        this.num\n",
    "    }\n",
    "    def copy(): AccumulatorV2[BigInt,BigInt] = {\n",
    "        new EvenAccumulator\n",
    "    }\n",
    "    def isZero():Boolean = {\n",
    "        this.num == 0\n",
    "    }\n",
    "}\n",
    "\n",
    "val acc = new EvenAccumulator\n",
    "val newAcc = sc.register(acc, \"evenAcc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89f58013-415b-4afb-bc7e-fdc73ee5df70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: BigInt = 0\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55ef2bd7-4b28-48fb-9ff1-9eb57dc1a696",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "java.lang.InternalError",
     "evalue": " java.lang.IllegalAccessException: final field has no write access: $Lambda$4208/0x00000008020f8208.arg$1/putField, from class java.lang.Object (module java.base)",
     "output_type": "error",
     "traceback": [
      "java.lang.InternalError: java.lang.IllegalAccessException: final field has no write access: $Lambda$4208/0x00000008020f8208.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:167)",
      "  at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:145)",
      "  at java.base/java.lang.reflect.Field.acquireOverrideFieldAccessor(Field.java:1184)",
      "  at java.base/java.lang.reflect.Field.getOverrideFieldAccessor(Field.java:1153)",
      "  at java.base/java.lang.reflect.Field.set(Field.java:820)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:406)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2501)",
      "  at org.apache.spark.rdd.RDD.$anonfun$foreach$1(RDD.scala:1002)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.RDD.foreach(RDD.scala:1001)",
      "  at org.apache.spark.sql.Dataset.$anonfun$foreach$1(Dataset.scala:3042)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:3845)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:3843)",
      "  at org.apache.spark.sql.Dataset.foreach(Dataset.scala:3042)",
      "  ... 34 elided",
      "Caused by: java.lang.IllegalAccessException: final field has no write access: $Lambda$4208/0x00000008020f8208.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectField(MethodHandles.java:3511)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectSetter(MethodHandles.java:3502)",
      "  at java.base/java.lang.invoke.MethodHandleImpl$1.unreflectField(MethodHandleImpl.java:1630)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:145)",
      "  ... 56 more",
      ""
     ]
    }
   ],
   "source": [
    "flights.foreach(flight_row => acc.add(flight_row.count))\n",
    "acc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f938fe8-c3da-4d75-996d-214a63e469f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
